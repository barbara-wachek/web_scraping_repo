{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RnMSKNXozi9",
        "outputId": "944f48e6-cef0-461a-8985-8a8f1ef33ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pydrive\n",
            "  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.12/dist-packages (from pydrive) (2.187.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.12/dist-packages (from pydrive) (6.0.3)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (0.31.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (2.43.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (0.3.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (2.29.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (4.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (4.9.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (1.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.72.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.27.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.2->pydrive) (6.2.4)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=1.2->pydrive) (3.3.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2026.1.4)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pydrive\n",
            "  Building wheel for pydrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pydrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27433 sha256=eceeca79b3eda6c281a46e7507596a632106423e3d4f8971c6103d73cf7668ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/10/da/a5b513f5b3916fc391c20ee7b4633e5cf3396d570cdd74970f\n",
            "Successfully built pydrive\n",
            "Installing collected packages: xlsxwriter, pydrive\n",
            "Successfully installed pydrive-1.3.1 xlsxwriter-3.2.9\n"
          ]
        }
      ],
      "source": [
        "!pip install xlsxwriter pydrive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%% import\n",
        "from __future__ import unicode_literals\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from time import mktime\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import json\n",
        "import xlsxwriter\n",
        "\n",
        "\n",
        "#%% functions - data conversion\n",
        "\n",
        "def date_change_format(date_string):\n",
        "    \"\"\"\n",
        "    Konwertuje datę z różnych formatów na \"YYYY-MM-DD\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "        date_string = ' '.join(date_string.strip().split())\n",
        "\n",
        "        if re.match(r'\\d{4}-\\d{2}-\\d{2}', date_string):\n",
        "            return date_string[:10]\n",
        "\n",
        "        if 'T' in date_string:\n",
        "            return date_string.split('T')[0]\n",
        "\n",
        "        if re.match(r'\\d{2}\\.\\d{2}\\.\\d{4}', date_string):\n",
        "            result = time.strptime(date_string, \"%d.%m.%Y\")\n",
        "            changed_date = datetime.fromtimestamp(mktime(result))\n",
        "            return format(changed_date.date())\n",
        "\n",
        "        # Blogger format: \"2024-12-15T10:30:00+01:00\"\n",
        "        if re.match(r'\\d{4}-\\d{2}-\\d{2}T', date_string):\n",
        "            return date_string.split('T')[0]\n",
        "\n",
        "        return \"no date\"\n",
        "    except Exception as e:\n",
        "        return \"no date\"\n",
        "\n",
        "\n",
        "#%% functions - link extraction\n",
        "\n",
        "def get_all_post_links():\n",
        "    \"\"\"\n",
        "    Pobiera wszystkie linki do postów z Blogspot\n",
        "    Używa Atom feed API\n",
        "    \"\"\"\n",
        "    base_url = \"https://cuda-cudanakiju.blogspot.com\"\n",
        "    all_links = []\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"KROK 1: Pobieranie linków z Blogspot\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Blogspot Atom feed - automatyczna paginacja\n",
        "    start_index = 1\n",
        "    max_results = 500  # Max na stronę\n",
        "    page = 1\n",
        "\n",
        "    while True:\n",
        "        feed_url = f\"{base_url}/feeds/posts/default?start-index={start_index}&max-results={max_results}&alt=json\"\n",
        "\n",
        "        try:\n",
        "            print(f\"Strona {page} (start-index={start_index})...\")\n",
        "            r = requests.get(feed_url, timeout=10)\n",
        "\n",
        "            if r.status_code == 200:\n",
        "                data = r.json()\n",
        "\n",
        "                # Sprawdź czy są wpisy\n",
        "                if 'feed' in data and 'entry' in data['feed']:\n",
        "                    entries = data['feed']['entry']\n",
        "\n",
        "                    # Wyciągnij linki\n",
        "                    for entry in entries:\n",
        "                        if 'link' in entry:\n",
        "                            for link in entry['link']:\n",
        "                                if link.get('rel') == 'alternate' and link.get('type') == 'text/html':\n",
        "                                    post_url = link['href']\n",
        "                                    all_links.append(post_url)\n",
        "\n",
        "                    print(f\"  ✓ Znaleziono {len(entries)} postów\")\n",
        "\n",
        "                    # Sprawdź czy są kolejne strony\n",
        "                    total_results = int(data['feed'].get('openSearch$totalResults', {}).get('$t', 0))\n",
        "                    items_per_page = int(data['feed'].get('openSearch$itemsPerPage', {}).get('$t', 0))\n",
        "\n",
        "                    if start_index + items_per_page >= total_results:\n",
        "                        print(f\"  → Koniec (łącznie {total_results} postów)\")\n",
        "                        break\n",
        "\n",
        "                    start_index += max_results\n",
        "                    page += 1\n",
        "                    time.sleep(0.5)\n",
        "                else:\n",
        "                    print(\"  → Brak więcej postów\")\n",
        "                    break\n",
        "            else:\n",
        "                print(f\"  ✗ Status {r.status_code}\")\n",
        "                break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Błąd: {e}\")\n",
        "            break\n",
        "\n",
        "    # Deduplikacja\n",
        "    all_links = list(set(all_links))\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Łącznie znaleziono: {len(all_links)} unikalnych postów\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "#%% functions - scraping\n",
        "\n",
        "def dictionary_of_article(article_link):\n",
        "    \"\"\"\n",
        "    Pobiera szczegóły artykułu z Blogspot\n",
        "    \"\"\"\n",
        "    try:\n",
        "        r = requests.get(article_link, timeout=15)\n",
        "        r.encoding = 'utf-8'\n",
        "\n",
        "        if r.status_code != 200:\n",
        "            errors.append(article_link)\n",
        "            return\n",
        "\n",
        "        soup = BeautifulSoup(r.text, 'lxml')\n",
        "\n",
        "        # Data publikacji\n",
        "        try:\n",
        "            # Blogspot używa <abbr> lub <time>\n",
        "            date_element = soup.find('abbr', class_='published')\n",
        "            if not date_element:\n",
        "                date_element = soup.find('time', class_='published')\n",
        "            if not date_element:\n",
        "                date_element = soup.find('span', class_='post-timestamp')\n",
        "            if not date_element:\n",
        "                # Meta tag\n",
        "                meta_date = soup.find('meta', property='article:published_time')\n",
        "                if meta_date:\n",
        "                    date_element = type('obj', (object,), {\n",
        "                        'get_text': lambda: meta_date.get('content', ''),\n",
        "                        'get': lambda x: meta_date.get('content', '')\n",
        "                    })()\n",
        "\n",
        "            if date_element:\n",
        "                date_text = date_element.get('title') or date_element.get('datetime') or date_element.get('content') or date_element.get_text(strip=True)\n",
        "                date_of_publication = date_change_format(date_text)\n",
        "            else:\n",
        "                date_of_publication = \"no date\"\n",
        "        except:\n",
        "            date_of_publication = \"no date\"\n",
        "\n",
        "        # Tytuł\n",
        "        try:\n",
        "            title_element = soup.find('h3', class_='post-title')\n",
        "            if not title_element:\n",
        "                title_element = soup.find('h1', class_='post-title')\n",
        "            if not title_element:\n",
        "                title_element = soup.find('h2', class_='post-title')\n",
        "            if not title_element:\n",
        "                title_element = soup.find('h1')\n",
        "            if not title_element:\n",
        "                title_element = soup.find('title')\n",
        "\n",
        "            if title_element:\n",
        "                title = title_element.get_text(strip=True)\n",
        "                # Usuń \"Cuda na kiju: \" lub podobne prefiksy\n",
        "                title = re.sub(r'^.*?:\\s*', '', title, count=1)\n",
        "                title = title.strip()\n",
        "            else:\n",
        "                title = \"no title\"\n",
        "        except:\n",
        "            title = \"no title\"\n",
        "\n",
        "        # Autor\n",
        "        try:\n",
        "            author_element = soup.find('span', class_='author')\n",
        "            if not author_element:\n",
        "                author_element = soup.find('span', class_='post-author')\n",
        "            if not author_element:\n",
        "                author_element = soup.find('a', rel='author')\n",
        "            if not author_element:\n",
        "                # Meta tag\n",
        "                meta_author = soup.find('meta', {'name': 'author'})\n",
        "                if meta_author:\n",
        "                    author_element = type('obj', (object,), {\n",
        "                        'get_text': lambda: meta_author.get('content', '')\n",
        "                    })()\n",
        "\n",
        "            if author_element:\n",
        "                author = author_element.get_text(strip=True)\n",
        "            else:\n",
        "                author = \"no author\"\n",
        "        except:\n",
        "            author = \"no author\"\n",
        "\n",
        "        # Treść artykułu\n",
        "        try:\n",
        "            # Blogspot używa różnych klas w zależności od szablonu\n",
        "            article_body = soup.find('div', class_='post-body')\n",
        "            if not article_body:\n",
        "                article_body = soup.find('div', class_='entry-content')\n",
        "            if not article_body:\n",
        "                article_body = soup.find('div', class_='post-content')\n",
        "            if not article_body:\n",
        "                # Fallback - szukamy article\n",
        "                article_elem = soup.find('article')\n",
        "                if article_elem:\n",
        "                    article_body = article_elem\n",
        "\n",
        "            if article_body:\n",
        "                text = article_body.get_text(separator=' ', strip=True)\n",
        "                text = text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
        "                text = re.sub(r'\\s+', ' ', text)\n",
        "            else:\n",
        "                text = \"no text\"\n",
        "        except:\n",
        "            text = \"no text\"\n",
        "\n",
        "        # Kategoria / Labels\n",
        "        try:\n",
        "            # Blogspot używa \"labels\" zamiast kategorii\n",
        "            label_elements = soup.find_all('a', rel='tag')\n",
        "            if not label_elements:\n",
        "                label_elements = soup.find_all('span', class_='post-labels')\n",
        "\n",
        "            if label_elements:\n",
        "                categories = []\n",
        "                for elem in label_elements:\n",
        "                    cat_text = elem.get_text(strip=True)\n",
        "                    if cat_text and cat_text not in categories:\n",
        "                        categories.append(cat_text)\n",
        "                category = ' | '.join(categories)\n",
        "            else:\n",
        "                category = \"no category\"\n",
        "        except:\n",
        "            category = \"no category\"\n",
        "\n",
        "        # Tagi - na Blogspot labels = kategorie = tagi\n",
        "        try:\n",
        "            label_elements = soup.find_all('a', rel='tag')\n",
        "            if label_elements:\n",
        "                tags = [tag.get_text(strip=True) for tag in label_elements]\n",
        "                tags_str = ' | '.join(tags)\n",
        "            else:\n",
        "                tags_str = None\n",
        "        except:\n",
        "            tags_str = None\n",
        "\n",
        "        # Linki zewnętrzne\n",
        "        try:\n",
        "            if article_body:\n",
        "                links = [a['href'] for a in article_body.find_all('a', href=True)]\n",
        "                external_links = [link for link in links if not re.search(r'blogspot\\.com', link) and link.startswith('http')]\n",
        "                external_links = ' | '.join(external_links) if external_links else None\n",
        "            else:\n",
        "                external_links = None\n",
        "        except:\n",
        "            external_links = None\n",
        "\n",
        "        # Zdjęcia\n",
        "        try:\n",
        "            images = []\n",
        "\n",
        "            if article_body:\n",
        "                for img in article_body.find_all('img'):\n",
        "                    img_url = img.get('src') or img.get('data-src')\n",
        "                    if img_url:\n",
        "                        # Pomijamy małe ikony\n",
        "                        if 'icon' not in img_url.lower() and 'emoji' not in img_url.lower():\n",
        "                            if img_url not in images:\n",
        "                                images.append(img_url)\n",
        "\n",
        "            has_images = len(images) > 0\n",
        "            photos_links = ' | '.join(images) if images else None\n",
        "        except:\n",
        "            has_images = False\n",
        "            photos_links = None\n",
        "\n",
        "        # Filmy\n",
        "        try:\n",
        "            if article_body:\n",
        "                iframes = [iframe['src'] for iframe in article_body.find_all('iframe', src=True)]\n",
        "                has_videos = len(iframes) > 0\n",
        "            else:\n",
        "                has_videos = False\n",
        "        except:\n",
        "            has_videos = False\n",
        "\n",
        "        result = {\n",
        "            \"Link\": article_link,\n",
        "            \"Data publikacji\": date_of_publication,\n",
        "            \"Tytuł artykułu\": title.replace('\\xa0', ' '),\n",
        "            \"Tekst artykułu\": text,\n",
        "            \"Autor\": author,\n",
        "            \"Kategoria\": category,\n",
        "            \"Tagi\": tags_str,\n",
        "            \"Linki zewnętrzne\": external_links,\n",
        "            \"Zdjęcia/Grafika\": has_images,\n",
        "            \"Filmy\": has_videos,\n",
        "            \"Linki do zdjęć\": photos_links\n",
        "        }\n",
        "\n",
        "        all_results.append(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        errors.append(article_link)\n",
        "\n",
        "\n",
        "#%% main execution\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SCRAPER CUDA-CUDANAKIJU.BLOGSPOT.COM\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # KROK 1: Pobierz linki\n",
        "    article_links = get_all_post_links()\n",
        "\n",
        "    if not article_links:\n",
        "        print(\"Nie znaleziono postów!\")\n",
        "        exit(1)\n",
        "\n",
        "    # KROK 2: Scrapuj artykuły\n",
        "    all_results = []\n",
        "    errors = []\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"KROK 2: Scraping artykułów\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    max_workers = 10\n",
        "    print(f\"Używam {max_workers} równoległych wątków\\n\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        list(tqdm(executor.map(dictionary_of_article, article_links), total=len(article_links)))\n",
        "\n",
        "    # KROK 3: Zapisz wyniki\n",
        "    timestamp = datetime.today().date()\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"KROK 3: Zapisywanie wyników\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # JSON\n",
        "    json_file = f'cuda_cudanakiju_{timestamp}.json'\n",
        "    with open(json_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)\n",
        "    print(f\"  ✓ {json_file}\")\n",
        "\n",
        "    # Excel\n",
        "    excel_file = f\"cuda_cudanakiju_{timestamp}.xlsx\"\n",
        "    df = pd.DataFrame(all_results)\n",
        "    with pd.ExcelWriter(excel_file,\n",
        "                       engine='xlsxwriter',\n",
        "                       engine_kwargs={'options': {'strings_to_urls': False}}) as writer:\n",
        "        df.to_excel(writer, 'Posts', index=False)\n",
        "    print(f\"  ✓ {excel_file}\")\n",
        "\n",
        "    # RAPORT KOŃCOWY\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"RAPORT KOŃCOWY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Pobranych artykułów: {len(all_results)}\")\n",
        "    print(f\"Błędów: {len(errors)}\")\n",
        "\n",
        "    if errors and len(errors) <= 10:\n",
        "        print(f\"\\nLinki z błędami:\")\n",
        "        for error_link in errors:\n",
        "            print(f\"  - {error_link}\")\n",
        "    elif errors:\n",
        "        print(f\"\\nLinki z błędami (pierwsze 10):\")\n",
        "        for error_link in errors[:10]:\n",
        "            print(f\"  - {error_link}\")\n",
        "        print(f\"  ... i {len(errors) - 10} więcej\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"GOTOWE!\")\n",
        "    print(\"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o7bsB6Eq7xC",
        "outputId": "1ce574ec-74bb-4cf5-8a67-8faf4d535b25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SCRAPER CUDA-CUDANAKIJU.BLOGSPOT.COM\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "KROK 1: Pobieranie linków z Blogspot\n",
            "============================================================\n",
            "\n",
            "Strona 1 (start-index=1)...\n",
            "  ✓ Znaleziono 150 postów\n",
            "Strona 2 (start-index=501)...\n",
            "  ✓ Znaleziono 150 postów\n",
            "  → Koniec (łącznie 790 postów)\n",
            "\n",
            "============================================================\n",
            "Łącznie znaleziono: 300 unikalnych postów\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "KROK 2: Scraping artykułów\n",
            "============================================================\n",
            "\n",
            "Używam 10 równoległych wątków\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [00:47<00:00,  6.38it/s]\n",
            "/tmp/ipython-input-1417306482.py:365: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  df.to_excel(writer, 'Posts', index=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "KROK 3: Zapisywanie wyników\n",
            "============================================================\n",
            "  ✓ cuda_cudanakiju_2026-01-13.json\n",
            "  ✓ cuda_cudanakiju_2026-01-13.xlsx\n",
            "\n",
            "============================================================\n",
            "RAPORT KOŃCOWY\n",
            "============================================================\n",
            "Pobranych artykułów: 300\n",
            "Błędów: 0\n",
            "\n",
            "============================================================\n",
            "GOTOWE!\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}