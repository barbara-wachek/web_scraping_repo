{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter pydrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ksAkyWjoMhS",
        "outputId": "055a2447-9a60-45c2-b748-e49fcb2143be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pydrive\n",
            "  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.12/dist-packages (from pydrive) (2.187.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.12/dist-packages (from pydrive) (6.0.3)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (0.31.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (2.43.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (0.3.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (2.29.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (4.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (4.9.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (1.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.72.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.27.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.2->pydrive) (6.2.4)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=1.2->pydrive) (3.3.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2026.1.4)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pydrive\n",
            "  Building wheel for pydrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pydrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27433 sha256=10cf3971926b649f5b0df005bce90a800c659f2fa5767345ac0069cf83e2e2b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/10/da/a5b513f5b3916fc391c20ee7b4633e5cf3396d570cdd74970f\n",
            "Successfully built pydrive\n",
            "Installing collected packages: xlsxwriter, pydrive\n",
            "Successfully installed pydrive-1.3.1 xlsxwriter-3.2.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%% import\n",
        "from __future__ import unicode_literals\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from time import mktime\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import json\n",
        "import xlsxwriter\n",
        "\n",
        "\n",
        "#%% functions - data conversion\n",
        "\n",
        "def date_change_format(date_string):\n",
        "    \"\"\"\n",
        "    Konwertuje datę z różnych formatów na \"YYYY-MM-DD\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "        date_string = ' '.join(date_string.strip().split())\n",
        "\n",
        "        if re.match(r'\\d{4}-\\d{2}-\\d{2}', date_string):\n",
        "            return date_string[:10]\n",
        "\n",
        "        if 'T' in date_string:\n",
        "            return date_string.split('T')[0]\n",
        "\n",
        "        if re.match(r'\\d{2}\\.\\d{2}\\.\\d{4}', date_string):\n",
        "            result = time.strptime(date_string, \"%d.%m.%Y\")\n",
        "            changed_date = datetime.fromtimestamp(mktime(result))\n",
        "            return format(changed_date.date())\n",
        "\n",
        "        lookup_table = {\n",
        "            \"stycznia\": \"01\", \"lutego\": \"02\", \"marca\": \"03\", \"kwietnia\": \"04\",\n",
        "            \"maja\": \"05\", \"czerwca\": \"06\", \"lipca\": \"07\", \"sierpnia\": \"08\",\n",
        "            \"września\": \"09\", \"października\": \"10\", \"listopada\": \"11\", \"grudnia\": \"12\",\n",
        "            \"styczeń\": \"01\", \"luty\": \"02\", \"marzec\": \"03\", \"kwiecień\": \"04\",\n",
        "            \"maj\": \"05\", \"czerwiec\": \"06\", \"lipiec\": \"07\", \"sierpień\": \"08\",\n",
        "            \"wrzesień\": \"09\", \"październik\": \"10\", \"listopad\": \"11\", \"grudzień\": \"12\"\n",
        "        }\n",
        "\n",
        "        for k, v in lookup_table.items():\n",
        "            date_string = date_string.replace(k, v)\n",
        "\n",
        "        if re.match(r'\\d{1,2}\\.\\d{1,2}\\.\\d{4}', date_string):\n",
        "            result = time.strptime(date_string, \"%d.%m.%Y\")\n",
        "        else:\n",
        "            result = time.strptime(date_string, \"%d %m %Y\")\n",
        "\n",
        "        changed_date = datetime.fromtimestamp(mktime(result))\n",
        "        return format(changed_date.date())\n",
        "    except Exception as e:\n",
        "        return \"no date\"\n",
        "\n",
        "\n",
        "#%% functions - link extraction\n",
        "\n",
        "def get_all_article_links():\n",
        "    \"\"\"\n",
        "    Pobiera wszystkie linki do artykułów z qfant.pl\n",
        "    Struktura: <article> z linkiem w <a href=\"/article-name.html\">\n",
        "    \"\"\"\n",
        "    base_url = \"https://www.qfant.pl\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"KROK 1: Pobieranie linków do artykułów\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    all_links = set()\n",
        "\n",
        "    # Kategorie do scrapowania (z menu głównego)\n",
        "    categories = {\n",
        "        'aktualności': '/cons/aktualnosci.html',\n",
        "        'artykuły': '/cons/artykuly.html',\n",
        "        'nowości wydawnicze': '/cons/nowosci-wydawnicze.html',\n",
        "        'patronaty': '/cons/patronaty.html',\n",
        "        'konkursy': '/cons/konkursy.html',\n",
        "        'wyniki': '/cons/wyniki.html',\n",
        "        'wywiady': '/cons/wywiady.html',\n",
        "        'wydarzenia': '/cons/wydarzenia.html',\n",
        "        'recenzje': '/cons/recenzja.html',\n",
        "    }\n",
        "\n",
        "    for cat_name, cat_url in categories.items():\n",
        "        print(f\"Pobieranie linków z kategorii: {cat_name}...\")\n",
        "\n",
        "        # qfant.pl ma paginację - próbujemy pobrać kilka pierwszych stron\n",
        "        for page in range(1, 6):  # pierwsze 5 stron każdej kategorii\n",
        "            try:\n",
        "                if page == 1:\n",
        "                    url = base_url + cat_url\n",
        "                else:\n",
        "                    # Paginacja to /page/2.html, /page/3.html itd.\n",
        "                    url = f\"{base_url}{cat_url.replace('.html', '')}/page/{page}.html\"\n",
        "\n",
        "                r = requests.get(url, timeout=10)\n",
        "                r.encoding = 'utf-8'\n",
        "\n",
        "                if r.status_code == 200:\n",
        "                    soup = BeautifulSoup(r.text, 'lxml')\n",
        "\n",
        "                    # Szukamy artykułów - <article> z linkami w tytule\n",
        "                    # <h5><a href=\"/copernicon-2016.html\">Copernicon 2016</a></h5>\n",
        "                    articles = soup.find_all('article', class_='post-item')\n",
        "\n",
        "                    page_links = 0\n",
        "                    for article in articles:\n",
        "                        # Znajdź link w tytule artykułu\n",
        "                        title_div = article.find('div', class_='post-title')\n",
        "                        if title_div:\n",
        "                            link = title_div.find('a')\n",
        "                            if link and link.get('href'):\n",
        "                                href = link['href']\n",
        "\n",
        "                                # Upewniamy się, że to pełny URL\n",
        "                                if href.startswith('http'):\n",
        "                                    full_url = href\n",
        "                                elif href.startswith('/'):\n",
        "                                    full_url = base_url + href\n",
        "                                else:\n",
        "                                    full_url = base_url + '/' + href\n",
        "\n",
        "                                # Dodajemy tylko linki do artykułów (kończy się na .html)\n",
        "                                if '.html' in full_url and '/page/' not in full_url:\n",
        "                                    all_links.add(full_url)\n",
        "                                    page_links += 1\n",
        "\n",
        "                    print(f\"  Strona {page}: {page_links} linków\")\n",
        "\n",
        "                    if page_links == 0:\n",
        "                        break  # Jeśli nie ma więcej artykułów, przerywamy paginację\n",
        "\n",
        "                    time.sleep(0.5)  # Aby nie przeciążać serwera\n",
        "\n",
        "                else:\n",
        "                    print(f\"  ✗ Status {r.status_code} dla strony {page}\")\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ✗ Błąd na stronie {page}: {e}\")\n",
        "                break\n",
        "\n",
        "        print(f\"  ✓ Zebrano linki z kategorii {cat_name}\\n\")\n",
        "\n",
        "    # Konwertujemy set na listę\n",
        "    all_links = list(all_links)\n",
        "\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Łącznie znaleziono: {len(all_links)} unikalnych artykułów\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "#%% functions - scraping\n",
        "\n",
        "def dictionary_of_article(article_link):\n",
        "    \"\"\"\n",
        "    Pobiera szczegóły artykułu z qfant.pl\n",
        "    \"\"\"\n",
        "    try:\n",
        "        r = requests.get(article_link, timeout=15)\n",
        "        r.encoding = 'utf-8'\n",
        "\n",
        "        if r.status_code != 200:\n",
        "            errors.append(article_link)\n",
        "            return\n",
        "\n",
        "        html_text = r.text\n",
        "\n",
        "        if 'error 404' in html_text.lower() or 'page not found' in html_text.lower():\n",
        "            errors.append(article_link)\n",
        "            return\n",
        "\n",
        "        soup = BeautifulSoup(html_text, 'lxml')\n",
        "\n",
        "        # Data publikacji\n",
        "        try:\n",
        "            # W HTML: <div class=\"post-date\">17 października 2016</div>\n",
        "            date_div = soup.find('div', class_='post-date')\n",
        "            if date_div:\n",
        "                date_text = date_div.get_text(strip=True)\n",
        "                date_of_publication = date_change_format(date_text)\n",
        "            else:\n",
        "                date_of_publication = \"no date\"\n",
        "        except:\n",
        "            date_of_publication = \"no date\"\n",
        "\n",
        "        # Tytuł\n",
        "        try:\n",
        "            # W HTML: <div class=\"title-caption-large\"><h3>Copernicon 2016</h3></div>\n",
        "            title_div = soup.find('div', class_='title-caption-large')\n",
        "            if title_div:\n",
        "                h3 = title_div.find('h3')\n",
        "                if h3:\n",
        "                    title = h3.get_text(strip=True)\n",
        "                else:\n",
        "                    title = \"no title\"\n",
        "            else:\n",
        "                title = \"no title\"\n",
        "        except:\n",
        "            title = \"no title\"\n",
        "\n",
        "        # Autor\n",
        "        try:\n",
        "            # Autor jest zwykle na końcu artykułu: \"Autor: Łukasz Szatkowski\"\n",
        "            author = \"no author\"\n",
        "            post_entry = soup.find('section', class_='post-entry')\n",
        "            if post_entry:\n",
        "                # Szukamy \"Autor:\" lub \"Recenzował:\" lub \"Recenzja:\"\n",
        "                text = post_entry.get_text()\n",
        "\n",
        "                # Wzorce dla autora\n",
        "                patterns = [\n",
        "                    r'Autor:\\s*([^\\n]+)',\n",
        "                    r'Recenzował[aą]?:\\s*([^\\n]+)',\n",
        "                    r'Recenzja:\\s*([^\\n]+)',\n",
        "                ]\n",
        "\n",
        "                for pattern in patterns:\n",
        "                    match = re.search(pattern, text, re.IGNORECASE)\n",
        "                    if match:\n",
        "                        author = match.group(1).strip()\n",
        "                        # Usuń ewentualne tagi HTML\n",
        "                        author = re.sub(r'<[^>]+>', '', author)\n",
        "                        break\n",
        "        except:\n",
        "            author = \"no author\"\n",
        "\n",
        "        # Treść artykułu\n",
        "        try:\n",
        "            # Główna treść jest w <section class=\"post-entry\">\n",
        "            post_entry = soup.find('section', class_='post-entry')\n",
        "\n",
        "            if post_entry:\n",
        "                # Usuń niepotrzebne elementy\n",
        "                for element in post_entry.find_all(['script', 'style', 'iframe', 'ins',\n",
        "                                                     'div']):\n",
        "                    # Zostawiamy paragrafy, ale usuwamy divy i inne\n",
        "                    if element.name == 'div' and element.get('class'):\n",
        "                        # Usuń divy z datą, tytułem, oceną\n",
        "                        if any(cls in ['post-date', 'post-title', 'star', 'clear']\n",
        "                               for cls in element.get('class', [])):\n",
        "                            element.decompose()\n",
        "\n",
        "                # Wyciągamy tekst z paragrafów\n",
        "                paragraphs = post_entry.find_all('p')\n",
        "                text_parts = []\n",
        "                for p in paragraphs:\n",
        "                    p_text = p.get_text(strip=True)\n",
        "                    # Pomijamy paragrafy z autorem (będzie osobno)\n",
        "                    if p_text and not any(skip in p_text for skip in\n",
        "                                        ['Autor:', 'Recenzował', 'Recenzja:']):\n",
        "                        text_parts.append(p_text)\n",
        "\n",
        "                text = ' '.join(text_parts)\n",
        "                text = text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
        "                text = re.sub(r'\\s+', ' ', text).strip()\n",
        "            else:\n",
        "                text = \"no text\"\n",
        "        except:\n",
        "            text = \"no text\"\n",
        "\n",
        "        # Kategoria - z tagów \"platform-teaser\"\n",
        "        try:\n",
        "            # <a style=\"background-color: #F18F19\" class=\"game_console\" href=\"/cons_artykuly.html\">Artykuły</a>\n",
        "            categories = []\n",
        "            platform_teaser = soup.find('div', class_='platform-teaser')\n",
        "            if platform_teaser:\n",
        "                links = platform_teaser.find_all('a', class_='game_console')\n",
        "                for link in links:\n",
        "                    cat_name = link.get_text(strip=True)\n",
        "                    if cat_name:\n",
        "                        categories.append(cat_name)\n",
        "\n",
        "            if categories:\n",
        "                category = ', '.join(categories)\n",
        "            else:\n",
        "                category = \"no category\"\n",
        "        except:\n",
        "            category = \"no category\"\n",
        "\n",
        "        # Tagi\n",
        "        tags_str = None\n",
        "\n",
        "        # Linki zewnętrzne\n",
        "        try:\n",
        "            post_entry = soup.find('section', class_='post-entry')\n",
        "            if post_entry:\n",
        "                links = [a['href'] for a in post_entry.find_all('a', href=True)]\n",
        "                external_links = [link for link in links\n",
        "                                if not re.search(r'qfant\\.pl', link)\n",
        "                                and link.startswith('http')]\n",
        "                external_links = ' | '.join(external_links) if external_links else None\n",
        "            else:\n",
        "                external_links = None\n",
        "        except:\n",
        "            external_links = None\n",
        "\n",
        "        # Zdjęcia\n",
        "        try:\n",
        "            images = []\n",
        "\n",
        "            # Główne zdjęcie artykułu - w <section class=\"post-thumb\">\n",
        "            post_thumb = soup.find('section', class_='post-thumb')\n",
        "            if post_thumb:\n",
        "                main_img = post_thumb.find('img')\n",
        "                if main_img and main_img.get('src'):\n",
        "                    img_url = main_img['src']\n",
        "                    if not img_url.startswith('http'):\n",
        "                        img_url = 'https://www.qfant.pl' + img_url\n",
        "                    if img_url not in images:\n",
        "                        images.append(img_url)\n",
        "\n",
        "            # Wszystkie zdjęcia z treści artykułu\n",
        "            post_entry = soup.find('section', class_='post-entry')\n",
        "            if post_entry:\n",
        "                all_imgs = post_entry.find_all('img')\n",
        "                for img in all_imgs:\n",
        "                    img_url = img.get('src') or img.get('data-lazy-src')\n",
        "                    if img_url:\n",
        "                        # Pomijamy małe ikony\n",
        "                        if not any(x in img_url.lower() for x in ['icon', 'logo', 'star-']):\n",
        "                            if not img_url.startswith('http'):\n",
        "                                img_url = 'https://www.qfant.pl' + img_url\n",
        "                            if img_url not in images:\n",
        "                                images.append(img_url)\n",
        "\n",
        "            has_images = len(images) > 0\n",
        "            photos_links = ' | '.join(images) if images else None\n",
        "        except:\n",
        "            has_images = False\n",
        "            photos_links = None\n",
        "\n",
        "        # Filmy\n",
        "        try:\n",
        "            has_videos = False\n",
        "            post_entry = soup.find('section', class_='post-entry')\n",
        "            if post_entry:\n",
        "                iframes = post_entry.find_all('iframe', src=True)\n",
        "                has_videos = len(iframes) > 0\n",
        "        except:\n",
        "            has_videos = False\n",
        "\n",
        "        result = {\n",
        "            \"Link\": article_link,\n",
        "            \"Data publikacji\": date_of_publication,\n",
        "            \"Tytuł artykułu\": title.replace('\\xa0', ' '),\n",
        "            \"Tekst artykułu\": text,\n",
        "            \"Autor\": author,\n",
        "            \"Kategoria\": category,\n",
        "            \"Tagi\": tags_str,\n",
        "            \"Linki zewnętrzne\": external_links,\n",
        "            \"Zdjęcia/Grafika\": has_images,\n",
        "            \"Filmy\": has_videos,\n",
        "            \"Linki do zdjęć\": photos_links\n",
        "        }\n",
        "\n",
        "        all_results.append(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        errors.append(article_link)\n",
        "\n",
        "\n",
        "#%% main execution\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SCRAPER QFANT.PL\")\n",
        "    print(\"Dwumiesięcznik fantastyczno-kryminalny\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # KROK 1: Pobierz linki do artykułów\n",
        "    article_links = get_all_article_links()\n",
        "\n",
        "    if not article_links:\n",
        "        print(\"Nie znaleziono artykułów!\")\n",
        "        exit(1)\n",
        "\n",
        "    # KROK 2: Scrapuj artykuły\n",
        "    all_results = []\n",
        "    errors = []\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"KROK 2: Scraping artykułów\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    max_workers = 10\n",
        "    print(f\"Używam {max_workers} równoległych wątków\\n\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        list(tqdm(executor.map(dictionary_of_article, article_links), total=len(article_links)))\n",
        "\n",
        "    # KROK 3: Zapisz wyniki\n",
        "    timestamp = datetime.today().date()\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"KROK 3: Zapisywanie wyników\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # JSON\n",
        "    json_file = f'qfant_{timestamp}.json'\n",
        "    with open(json_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)\n",
        "    print(f\"  ✓ {json_file}\")\n",
        "\n",
        "    # Excel\n",
        "    excel_file = f\"qfant_{timestamp}.xlsx\"\n",
        "    df = pd.DataFrame(all_results)\n",
        "    with pd.ExcelWriter(excel_file,\n",
        "                       engine='xlsxwriter',\n",
        "                       engine_kwargs={'options': {'strings_to_urls': False}}) as writer:\n",
        "        df.to_excel(writer, 'Posts', index=False)\n",
        "    print(f\"  ✓ {excel_file}\")\n",
        "\n",
        "    # RAPORT KOŃCOWY\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"RAPORT KOŃCOWY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Pobranych artykułów: {len(all_results)}\")\n",
        "    print(f\"Błędów: {len(errors)}\")\n",
        "\n",
        "    if errors and len(errors) <= 10:\n",
        "        print(f\"\\nLinki z błędami:\")\n",
        "        for error_link in errors:\n",
        "            print(f\"  - {error_link}\")\n",
        "    elif errors:\n",
        "        print(f\"\\nLinki z błędami (pierwsze 10):\")\n",
        "        for error_link in errors[:10]:\n",
        "            print(f\"  - {error_link}\")\n",
        "        print(f\"  ... i {len(errors) - 10} więcej\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"GOTOWE!\")\n",
        "    print(\"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx3jF1Cdt8QP",
        "outputId": "5d55e458-e13c-4824-8572-166856e503c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SCRAPER QFANT.PL\n",
            "Dwumiesięcznik fantastyczno-kryminalny\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "KROK 1: Pobieranie linków do artykułów\n",
            "============================================================\n",
            "\n",
            "Pobieranie linków z kategorii: aktualności...\n",
            "  Strona 1: 15 linków\n",
            "  Strona 2: 15 linków\n",
            "  Strona 3: 15 linków\n",
            "  Strona 4: 14 linków\n",
            "  Strona 5: 14 linków\n",
            "  ✓ Zebrano linki z kategorii aktualności\n",
            "\n",
            "Pobieranie linków z kategorii: artykuły...\n",
            "  Strona 1: 15 linków\n",
            "  Strona 2: 15 linków\n",
            "  Strona 3: 15 linków\n",
            "  Strona 4: 15 linków\n",
            "  Strona 5: 7 linków\n",
            "  ✓ Zebrano linki z kategorii artykuły\n",
            "\n",
            "Pobieranie linków z kategorii: nowości wydawnicze...\n",
            "  Strona 1: 15 linków\n",
            "  Strona 2: 12 linków\n",
            "  Strona 3: 15 linków\n",
            "  Strona 4: 15 linków\n",
            "  Strona 5: 13 linków\n",
            "  ✓ Zebrano linki z kategorii nowości wydawnicze\n",
            "\n",
            "Pobieranie linków z kategorii: patronaty...\n",
            "  Strona 1: 15 linków\n",
            "  Strona 2: 12 linków\n",
            "  Strona 3: 14 linków\n",
            "  Strona 4: 14 linków\n",
            "  Strona 5: 14 linków\n",
            "  ✓ Zebrano linki z kategorii patronaty\n",
            "\n",
            "Pobieranie linków z kategorii: konkursy...\n",
            "  Strona 1: 9 linków\n",
            "  Strona 2: 13 linków\n",
            "  Strona 3: 15 linków\n",
            "  Strona 4: 13 linków\n",
            "  Strona 5: 14 linków\n",
            "  ✓ Zebrano linki z kategorii konkursy\n",
            "\n",
            "Pobieranie linków z kategorii: wyniki...\n",
            "  Strona 1: 11 linków\n",
            "  Strona 2: 15 linków\n",
            "  Strona 3: 13 linków\n",
            "  Strona 4: 14 linków\n",
            "  Strona 5: 15 linków\n",
            "  ✓ Zebrano linki z kategorii wyniki\n",
            "\n",
            "Pobieranie linków z kategorii: wywiady...\n",
            "  Strona 1: 15 linków\n",
            "  Strona 2: 2 linków\n",
            "  ✗ Status 410 dla strony 3\n",
            "  ✓ Zebrano linki z kategorii wywiady\n",
            "\n",
            "Pobieranie linków z kategorii: wydarzenia...\n",
            "  Strona 1: 15 linków\n",
            "  Strona 2: 14 linków\n",
            "  Strona 3: 15 linków\n",
            "  Strona 4: 15 linków\n",
            "  Strona 5: 7 linków\n",
            "  ✓ Zebrano linki z kategorii wydarzenia\n",
            "\n",
            "Pobieranie linków z kategorii: recenzje...\n",
            "  Strona 1: 15 linków\n",
            "  Strona 2: 15 linków\n",
            "  Strona 3: 15 linków\n",
            "  Strona 4: 15 linków\n",
            "  Strona 5: 15 linków\n",
            "  ✓ Zebrano linki z kategorii recenzje\n",
            "\n",
            "============================================================\n",
            "Łącznie znaleziono: 479 unikalnych artykułów\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "KROK 2: Scraping artykułów\n",
            "============================================================\n",
            "\n",
            "Używam 10 równoległych wątków\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 479/479 [00:48<00:00,  9.87it/s]\n",
            "/tmp/ipython-input-1050694422.py:416: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  df.to_excel(writer, 'Posts', index=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "KROK 3: Zapisywanie wyników\n",
            "============================================================\n",
            "  ✓ qfant_2026-01-13.json\n",
            "  ✓ qfant_2026-01-13.xlsx\n",
            "\n",
            "============================================================\n",
            "RAPORT KOŃCOWY\n",
            "============================================================\n",
            "Pobranych artykułów: 411\n",
            "Błędów: 68\n",
            "\n",
            "Linki z błędami (pierwsze 10):\n",
            "  - https://www.qfant.pl/qfantydzien-z-prawem-do-zemsty-01-wyniki.html\n",
            "  - https://www.qfant.pl/qfantydzien-z-krucjata-05.html\n",
            "  - https://www.qfant.pl/karciany-qfantydzien-01.html\n",
            "  - https://www.qfant.pl/13746-2.html\n",
            "  - https://www.qfant.pl/grzmiacy-qfantydzien-04-wyniki.html\n",
            "  - https://www.qfant.pl/kwantowy-zlodziej-hannu-rajaniemi.html\n",
            "  - https://www.qfant.pl/qfantydzien-ze-sciana-burz-01-03-05-i-07-wyniki.html\n",
            "  - https://www.qfant.pl/qfantydzien-z-andromeda-05-wyniki.html\n",
            "  - https://www.qfant.pl/warcraft-narodziny-hordy-premiera.html\n",
            "  - https://www.qfant.pl/qfantydzien-z-andromeda-03-2.html\n",
            "  ... i 58 więcej\n",
            "\n",
            "============================================================\n",
            "GOTOWE!\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "i3TE6AphvAih",
        "outputId": "9f9c5bb9-aa30-4cf7-99c1-1fdeddd7f576"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Link Data publikacji  \\\n",
              "0  https://www.qfant.pl/review/tom-rob-smith-farm...      2015-11-11   \n",
              "1  https://www.qfant.pl/review/wilcza-godzina-and...      2017-07-25   \n",
              "2      https://www.qfant.pl/j-d-bujak-ogniskowa.html      2016-09-22   \n",
              "3  https://www.qfant.pl/review/vittorio-sgarbi-ob...      2016-01-03   \n",
              "4                 https://www.qfant.pl/qfant-14.html      2011-03-01   \n",
              "\n",
              "                              Tytuł artykułu  \\\n",
              "0                      Tom Rob Smith „Farma”   \n",
              "1          „Wilcza godzina”, Andrius Tapinas   \n",
              "2                   J. D. Bujak, „Ogniskowa”   \n",
              "3  Vittorio Sgarbi „Oblicza kobiet w sztuce”   \n",
              "4                                  QFANT #14   \n",
              "\n",
              "                                      Tekst artykułu           Autor  \\\n",
              "0  Tom Rob Smith to autor rewelacyjnego „Systemu”...   Damian Drabik   \n",
              "1  Nieczęsto na polskim rynku mamy do czynienia z...  Andrus Tapinas   \n",
              "2  Emilia jest z pozoru zwyczajną, skromną studen...       no author   \n",
              "3  Nie ulega wątpliwości, że najdoskonalszą chwil...   Damian Drabik   \n",
              "4  ŚCIĄGNIJ PDF Od dziś możecie się zapoznać z na...       no author   \n",
              "\n",
              "                       Kategoria  Tagi Linki zewnętrzne  Zdjęcia/Grafika  \\\n",
              "0                       Recenzja  None             None             True   \n",
              "1                       Recenzja  None             None             True   \n",
              "2  Nowości wydawnicze, Patronaty  None             None             True   \n",
              "3                       Recenzja  None             None             True   \n",
              "4        Aktualności, Wydarzenia  None             None             True   \n",
              "\n",
              "   Filmy                                     Linki do zdjęć  \n",
              "0  False  https://www.qfant.pl/wp-content/uploads/2015/1...  \n",
              "1  False  https://www.qfant.pl/wp-content/uploads/2017/0...  \n",
              "2  False  https://www.qfant.pl/wp-content/uploads/2016/0...  \n",
              "3  False  https://www.qfant.pl/wp-content/uploads/2016/0...  \n",
              "4  False  https://www.qfant.pl/wp-content/uploads/2013/0...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c1706d54-79d7-4ce3-894e-2374b8cead8c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Link</th>\n",
              "      <th>Data publikacji</th>\n",
              "      <th>Tytuł artykułu</th>\n",
              "      <th>Tekst artykułu</th>\n",
              "      <th>Autor</th>\n",
              "      <th>Kategoria</th>\n",
              "      <th>Tagi</th>\n",
              "      <th>Linki zewnętrzne</th>\n",
              "      <th>Zdjęcia/Grafika</th>\n",
              "      <th>Filmy</th>\n",
              "      <th>Linki do zdjęć</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.qfant.pl/review/tom-rob-smith-farm...</td>\n",
              "      <td>2015-11-11</td>\n",
              "      <td>Tom Rob Smith „Farma”</td>\n",
              "      <td>Tom Rob Smith to autor rewelacyjnego „Systemu”...</td>\n",
              "      <td>Damian Drabik</td>\n",
              "      <td>Recenzja</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>https://www.qfant.pl/wp-content/uploads/2015/1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.qfant.pl/review/wilcza-godzina-and...</td>\n",
              "      <td>2017-07-25</td>\n",
              "      <td>„Wilcza godzina”, Andrius Tapinas</td>\n",
              "      <td>Nieczęsto na polskim rynku mamy do czynienia z...</td>\n",
              "      <td>Andrus Tapinas</td>\n",
              "      <td>Recenzja</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>https://www.qfant.pl/wp-content/uploads/2017/0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.qfant.pl/j-d-bujak-ogniskowa.html</td>\n",
              "      <td>2016-09-22</td>\n",
              "      <td>J. D. Bujak, „Ogniskowa”</td>\n",
              "      <td>Emilia jest z pozoru zwyczajną, skromną studen...</td>\n",
              "      <td>no author</td>\n",
              "      <td>Nowości wydawnicze, Patronaty</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>https://www.qfant.pl/wp-content/uploads/2016/0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.qfant.pl/review/vittorio-sgarbi-ob...</td>\n",
              "      <td>2016-01-03</td>\n",
              "      <td>Vittorio Sgarbi „Oblicza kobiet w sztuce”</td>\n",
              "      <td>Nie ulega wątpliwości, że najdoskonalszą chwil...</td>\n",
              "      <td>Damian Drabik</td>\n",
              "      <td>Recenzja</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>https://www.qfant.pl/wp-content/uploads/2016/0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.qfant.pl/qfant-14.html</td>\n",
              "      <td>2011-03-01</td>\n",
              "      <td>QFANT #14</td>\n",
              "      <td>ŚCIĄGNIJ PDF Od dziś możecie się zapoznać z na...</td>\n",
              "      <td>no author</td>\n",
              "      <td>Aktualności, Wydarzenia</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>https://www.qfant.pl/wp-content/uploads/2013/0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1706d54-79d7-4ce3-894e-2374b8cead8c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c1706d54-79d7-4ce3-894e-2374b8cead8c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c1706d54-79d7-4ce3-894e-2374b8cead8c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}