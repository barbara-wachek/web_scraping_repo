{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMu79OcHyIbC"
      },
      "outputs": [],
      "source": [
        "!pip install xlsxwriter pydrive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%% import\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "#%% functions\n",
        "\n",
        "def get_posts_from_feed(feed_url, start_index=1, max_results=500):\n",
        "    \"\"\"\n",
        "    Pobiera posty z Blogger Atom feed\n",
        "    Blogger feed: /feeds/posts/default?start-index=X&max-results=Y\n",
        "    \"\"\"\n",
        "    posts = []\n",
        "    try:\n",
        "        # Dodaj parametry do URL\n",
        "        url = f\"{feed_url}?start-index={start_index}&max-results={max_results}&alt=rss\"\n",
        "\n",
        "        r = requests.get(url)\n",
        "        r.encoding = 'utf-8'\n",
        "        soup = BeautifulSoup(r.text, 'xml')\n",
        "\n",
        "        # W Atom feed linki są w <link> z rel=\"alternate\"\n",
        "        for item in soup.find_all('item'):\n",
        "            link_elem = item.find('link')\n",
        "            if link_elem:\n",
        "                link = link_elem.text.strip()\n",
        "                if link and 'blogspot.com' in link:\n",
        "                    posts.append(link)\n",
        "\n",
        "        # Alternatywnie próbuj <entry> (Atom format)\n",
        "        if not posts:\n",
        "            for entry in soup.find_all('entry'):\n",
        "                for link in entry.find_all('link', rel='alternate'):\n",
        "                    href = link.get('href')\n",
        "                    if href and 'blogspot.com' in href:\n",
        "                        posts.append(href)\n",
        "\n",
        "        return posts\n",
        "    except Exception as e:\n",
        "        print(f\"Błąd pobierania feed: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def get_all_posts(blog_url):\n",
        "    \"\"\"\n",
        "    Pobiera wszystkie posty z bloga Blogger używając feed\n",
        "    \"\"\"\n",
        "    feed_url = blog_url.rstrip('/') + '/feeds/posts/default'\n",
        "\n",
        "    print(f\"Używam feed: {feed_url}\")\n",
        "\n",
        "    all_posts = []\n",
        "    start_index = 1\n",
        "    batch_size = 500  # Maksymalny rozmiar batcha dla Blogger\n",
        "\n",
        "    print(\"\\nPobieranie postów z feed...\")\n",
        "\n",
        "    while True:\n",
        "        print(f\"  Pobieranie od pozycji {start_index}...\")\n",
        "        posts = get_posts_from_feed(feed_url, start_index, batch_size)\n",
        "\n",
        "        if not posts:\n",
        "            print(f\"  Brak więcej postów\")\n",
        "            break\n",
        "\n",
        "        all_posts.extend(posts)\n",
        "        print(f\"  Pobrano {len(posts)} postów\")\n",
        "\n",
        "        # Jeśli otrzymaliśmy mniej niż batch_size, to koniec\n",
        "        if len(posts) < batch_size:\n",
        "            break\n",
        "\n",
        "        start_index += batch_size\n",
        "        time.sleep(0.5)  # Ostrożność\n",
        "\n",
        "    # Usuń duplikaty\n",
        "    all_posts = list(set(all_posts))\n",
        "\n",
        "    return all_posts\n",
        "\n",
        "\n",
        "def filter_article_links(all_links):\n",
        "    \"\"\"\n",
        "    Filtruje linki - zostawia tylko artykuły\n",
        "    \"\"\"\n",
        "    article_links = []\n",
        "\n",
        "    for link in all_links:\n",
        "        # Wykluczamy strony statyczne i inne nie-posty\n",
        "        # Blogger posty zwykle mają /YYYY/MM/ w URL lub /.html\n",
        "\n",
        "        # Wykluczamy:\n",
        "        if '/search/' in link:  # Wyszukiwanie\n",
        "            continue\n",
        "        if '/p/' in link:  # Statyczne strony\n",
        "            continue\n",
        "        if link.endswith('.com/') or link.endswith('.com'):  # Główna strona\n",
        "            continue\n",
        "\n",
        "        # Akceptujemy posty\n",
        "        article_links.append(link)\n",
        "\n",
        "    return article_links\n",
        "\n",
        "\n",
        "#%% main execution\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    blog_url = \"https://jacekglomb.blogspot.com/\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Pobieranie linków z jacekglomb.blogspot.com\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Pobierz wszystkie posty\n",
        "    all_posts = get_all_posts(blog_url)\n",
        "\n",
        "    print(f\"\\nZnaleziono {len(all_posts)} postów z feed\")\n",
        "\n",
        "    # Filtruj\n",
        "    article_links = filter_article_links(all_posts)\n",
        "\n",
        "    print(f\"Po filtrowaniu: {len(article_links)} artykułów\")\n",
        "\n",
        "    if not article_links:\n",
        "        print(\"\\n⚠️  Nie znaleziono artykułów!\")\n",
        "        print(\"Sprawdź ręcznie feed:\")\n",
        "        print(f\"  {blog_url}feeds/posts/default\")\n",
        "        exit(1)\n",
        "\n",
        "    # Pokaż przykłady\n",
        "    if article_links:\n",
        "        print(\"\\nPrzykładowe linki (pierwsze 5):\")\n",
        "        for i, link in enumerate(article_links[:5], 1):\n",
        "            print(f\"  {i}. {link}\")\n",
        "\n",
        "    # Sortuj\n",
        "    article_links.sort()\n",
        "\n",
        "    # Zapisz do pliku tekstowego\n",
        "    with open('glomb_linki.txt', 'w', encoding='utf-8') as f:\n",
        "        for link in article_links:\n",
        "            f.write(link + '\\n')\n",
        "\n",
        "    # Zapisz do JSON\n",
        "    output_data = {\n",
        "        'source': blog_url,\n",
        "        'feed': blog_url + 'feeds/posts/default',\n",
        "        'total_links': len(article_links),\n",
        "        'links': article_links,\n",
        "        'errors': []\n",
        "    }\n",
        "\n",
        "    with open('glomb_linki.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Raport\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RAPORT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Znaleziono artykułów: {len(article_links)}\")\n",
        "    print(f\"\\nZapisano do:\")\n",
        "    print(f\"  - glomb_linki.txt\")\n",
        "    print(f\"  - glomb_linki.json\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qsgOOkA1yAb",
        "outputId": "f301fa30-0b2a-429f-b619-f4ac93c2eb1e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Pobieranie linków z jacekglomb.blogspot.com\n",
            "============================================================\n",
            "Używam feed: https://jacekglomb.blogspot.com/feeds/posts/default\n",
            "\n",
            "Pobieranie postów z feed...\n",
            "  Pobieranie od pozycji 1...\n",
            "  Pobrano 149 postów\n",
            "\n",
            "Znaleziono 149 postów z feed\n",
            "Po filtrowaniu: 149 artykułów\n",
            "\n",
            "Przykładowe linki (pierwsze 5):\n",
            "  1. https://jacekglomb.blogspot.com/2013/06/jak-zostaem-kibicem-arsenalu-czesc-ii.html\n",
            "  2. https://jacekglomb.blogspot.com/2012/06/czesi-wola-hokej.html\n",
            "  3. https://jacekglomb.blogspot.com/2011/09/orkiestra.html\n",
            "  4. https://jacekglomb.blogspot.com/2014/02/do-marka-fiedora-sow-kilka_18.html\n",
            "  5. https://jacekglomb.blogspot.com/2012/10/zapach-zuzla.html\n",
            "\n",
            "============================================================\n",
            "RAPORT\n",
            "============================================================\n",
            "Znaleziono artykułów: 149\n",
            "\n",
            "Zapisano do:\n",
            "  - glomb_linki.txt\n",
            "  - glomb_linki.json\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%% import\n",
        "from __future__ import unicode_literals\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from time import mktime\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import json\n",
        "import xlsxwriter\n",
        "\n",
        "\n",
        "#%% functions\n",
        "\n",
        "def date_change_format(date_string):\n",
        "    \"\"\"\n",
        "    Konwertuje datę z różnych formatów na \"YYYY-MM-DD\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "        date_string = ' '.join(date_string.strip().split())\n",
        "\n",
        "        if re.match(r'\\d{4}-\\d{2}-\\d{2}', date_string):\n",
        "            return date_string[:10]\n",
        "\n",
        "        if 'T' in date_string:\n",
        "            return date_string.split('T')[0]\n",
        "\n",
        "        lookup_table = {\n",
        "            \"stycznia\": \"01\", \"lutego\": \"02\", \"marca\": \"03\", \"kwietnia\": \"04\",\n",
        "            \"maja\": \"05\", \"czerwca\": \"06\", \"lipca\": \"07\", \"sierpnia\": \"08\",\n",
        "            \"września\": \"09\", \"października\": \"10\", \"listopada\": \"11\", \"grudnia\": \"12\",\n",
        "            \"styczeń\": \"01\", \"luty\": \"02\", \"marzec\": \"03\", \"kwiecień\": \"04\",\n",
        "            \"maj\": \"05\", \"czerwiec\": \"06\", \"lipiec\": \"07\", \"sierpień\": \"08\",\n",
        "            \"wrzesień\": \"09\", \"październik\": \"10\", \"listopad\": \"11\", \"grudzień\": \"12\"\n",
        "        }\n",
        "\n",
        "        for k, v in lookup_table.items():\n",
        "            date_string = date_string.replace(k, v)\n",
        "\n",
        "        if re.match(r'\\d{1,2}\\.\\d{1,2}\\.\\d{4}', date_string):\n",
        "            result = time.strptime(date_string, \"%d.%m.%Y\")\n",
        "        else:\n",
        "            result = time.strptime(date_string, \"%d %m %Y\")\n",
        "\n",
        "        changed_date = datetime.fromtimestamp(mktime(result))\n",
        "        return format(changed_date.date())\n",
        "    except Exception as e:\n",
        "        print(f\"Błąd konwersji daty '{date_string}': {e}\")\n",
        "        return \"no date\"\n",
        "\n",
        "\n",
        "def dictionary_of_article(article_link):\n",
        "    \"\"\"\n",
        "    Pobiera szczegóły artykułu z jacekglomb.blogspot.com (Blogger)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        r = requests.get(article_link)\n",
        "        r.encoding = 'utf-8'\n",
        "        html_text = r.text\n",
        "\n",
        "        while '429 Too Many Requests' in html_text:\n",
        "            time.sleep(5)\n",
        "            r = requests.get(article_link)\n",
        "            r.encoding = 'utf-8'\n",
        "            html_text = r.text\n",
        "\n",
        "        soup = BeautifulSoup(html_text, 'lxml')\n",
        "\n",
        "        # Data publikacji - Blogger używa różnych struktur\n",
        "        try:\n",
        "            # Opcja 1: <time> lub <abbr class=\"published\">\n",
        "            date_element = soup.find('time')\n",
        "            if not date_element:\n",
        "                date_element = soup.find('abbr', class_='published')\n",
        "            if not date_element:\n",
        "                date_element = soup.find('span', class_=lambda x: x and 'date' in str(x).lower())\n",
        "\n",
        "            if date_element:\n",
        "                date_text = date_element.get('datetime') or date_element.get('title') or date_element.get_text(strip=True)\n",
        "                date_of_publication = date_change_format(date_text)\n",
        "            else:\n",
        "                date_of_publication = \"no date\"\n",
        "        except Exception as e:\n",
        "            print(f\"Błąd parsowania daty dla {article_link}: {e}\")\n",
        "            date_of_publication = \"no date\"\n",
        "\n",
        "        # Tytuł - Blogger\n",
        "        try:\n",
        "            title_element = soup.find('h1', class_=lambda x: x and 'title' in str(x).lower())\n",
        "            if not title_element:\n",
        "                title_element = soup.find('h3', class_=lambda x: x and 'post-title' in str(x).lower())\n",
        "            if not title_element:\n",
        "                title_element = soup.find('h1')\n",
        "\n",
        "            title = title_element.get_text(strip=True) if title_element else \"no title\"\n",
        "        except:\n",
        "            title = \"no title\"\n",
        "\n",
        "        # Autor - Blogger\n",
        "        try:\n",
        "            author_element = soup.find('span', class_=lambda x: x and 'author' in str(x).lower())\n",
        "            if not author_element:\n",
        "                author_element = soup.find('a', rel='author')\n",
        "            if not author_element:\n",
        "                author_element = soup.find('span', itemprop='name')\n",
        "\n",
        "            if author_element:\n",
        "                author = author_element.get_text(strip=True)\n",
        "                author = re.sub(r'^(Autor|By|Opublikował|Posted by):\\s*', '', author, flags=re.IGNORECASE)\n",
        "            else:\n",
        "                # Domyślnie Jacek Głomb (to jego blog)\n",
        "                author = \"Jacek Głomb\"\n",
        "        except:\n",
        "            author = \"Jacek Głomb\"\n",
        "\n",
        "        # Treść artykułu - Blogger\n",
        "        try:\n",
        "            # Blogger używa różnych klas\n",
        "            article_body = soup.find('div', class_=lambda x: x and 'post-body' in str(x).lower())\n",
        "            if not article_body:\n",
        "                article_body = soup.find('div', class_=lambda x: x and 'entry-content' in str(x).lower())\n",
        "            if not article_body:\n",
        "                article_body = soup.find('article')\n",
        "\n",
        "            if article_body:\n",
        "                text = article_body.get_text(strip=True).replace('\\n', ' ').replace('\\xa0', ' ')\n",
        "            else:\n",
        "                text = \"no text\"\n",
        "        except:\n",
        "            text = \"no text\"\n",
        "\n",
        "        # Kategorie/Labels - Blogger\n",
        "        try:\n",
        "            # Blogger nazywa to \"labels\"\n",
        "            label_links = soup.find_all('a', rel='tag')\n",
        "            if not label_links:\n",
        "                label_links = soup.find_all('a', class_=lambda x: x and 'label' in str(x).lower())\n",
        "\n",
        "            if label_links:\n",
        "                categories = [cat.get_text(strip=True) for cat in label_links]\n",
        "                category = ' | '.join(categories)\n",
        "            else:\n",
        "                category = \"no category\"\n",
        "        except:\n",
        "            category = \"no category\"\n",
        "\n",
        "        # Linki zewnętrzne\n",
        "        try:\n",
        "            if article_body:\n",
        "                links = [a['href'] for a in article_body.find_all('a', href=True)]\n",
        "                external_links = [link for link in links if not re.search(r'blogspot\\.com|blogger\\.com', link)]\n",
        "                external_links = ' | '.join(external_links) if external_links else None\n",
        "            else:\n",
        "                external_links = None\n",
        "        except (AttributeError, KeyError, IndexError):\n",
        "            external_links = None\n",
        "\n",
        "        # Zdjęcia\n",
        "        try:\n",
        "            images = []\n",
        "\n",
        "            # 1. Zdjęcia w treści\n",
        "            if article_body:\n",
        "                content_images = [img['src'] for img in article_body.find_all('img', src=True) if img.get('src')]\n",
        "                for img_src in content_images:\n",
        "                    if img_src not in images:\n",
        "                        images.append(img_src)\n",
        "\n",
        "            # 2. Inne miejsca\n",
        "            for container_class in ['post-header', 'entry-header']:\n",
        "                header = soup.find('div', class_=container_class)\n",
        "                if header:\n",
        "                    header_images = [img['src'] for img in header.find_all('img', src=True) if img.get('src')]\n",
        "                    for img_src in header_images:\n",
        "                        if img_src not in images:\n",
        "                            images.append(img_src)\n",
        "\n",
        "            has_images = len(images) > 0\n",
        "            photos_links = ' | '.join(images) if images else None\n",
        "        except (AttributeError, KeyError, IndexError):\n",
        "            has_images = False\n",
        "            photos_links = None\n",
        "\n",
        "        # Filmy (iframe)\n",
        "        try:\n",
        "            if article_body:\n",
        "                iframes = [iframe['src'] for iframe in article_body.find_all('iframe', src=True)]\n",
        "                has_videos = len(iframes) > 0\n",
        "            else:\n",
        "                has_videos = False\n",
        "        except:\n",
        "            has_videos = False\n",
        "\n",
        "        dictionary_of_article = {\n",
        "            \"Link\": article_link,\n",
        "            \"Data publikacji\": date_of_publication,\n",
        "            \"Tytuł artykułu\": title.replace('\\xa0', ' '),\n",
        "            \"Tekst artykułu\": text,\n",
        "            \"Autor\": author,\n",
        "            \"Kategoria\": category,\n",
        "            \"Linki zewnętrzne\": external_links,\n",
        "            \"Zdjęcia/Grafika\": has_images,\n",
        "            \"Filmy\": has_videos,\n",
        "            \"Linki do zdjęć\": photos_links\n",
        "        }\n",
        "\n",
        "        all_results.append(dictionary_of_article)\n",
        "\n",
        "    except AttributeError as e:\n",
        "        errors.append(article_link)\n",
        "        print(f\"Błąd dla {article_link}: {e}\")\n",
        "    except Exception as e:\n",
        "        errors.append(article_link)\n",
        "        print(f\"Nieoczekiwany błąd dla {article_link}: {e}\")\n",
        "\n",
        "\n",
        "#%% main execution\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Wczytaj linki z pliku\n",
        "    try:\n",
        "        with open('glomb_linki.txt', 'r', encoding='utf-8') as f:\n",
        "            article_links = [line.strip() for line in f if line.strip()]\n",
        "        print(f\"Wczytano {len(article_links)} linków z pliku\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Nie znaleziono pliku glomb_linki.txt\")\n",
        "        print(\"Użyj najpierw get_glomb_links.py aby pobrać linki!\")\n",
        "        article_links = []\n",
        "\n",
        "    if not article_links:\n",
        "        print(\"Brak linków do przetworzenia!\")\n",
        "        exit(1)\n",
        "\n",
        "    all_results = []\n",
        "    errors = []\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Rozpoczynam scraping artykułów z jacekglomb.blogspot.com\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Scraping z progress barem\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        list(tqdm(executor.map(dictionary_of_article, article_links), total=len(article_links)))\n",
        "\n",
        "    # Zapisywanie wyników\n",
        "    timestamp = datetime.today().date()\n",
        "\n",
        "    # JSON\n",
        "    with open(f'glomb_{timestamp}.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)\n",
        "\n",
        "    # Excel\n",
        "    df = pd.DataFrame(all_results)\n",
        "    with pd.ExcelWriter(f\"glomb_{timestamp}.xlsx\",\n",
        "                       engine='xlsxwriter',\n",
        "                       engine_kwargs={'options': {'strings_to_urls': False}}) as writer:\n",
        "        df.to_excel(writer, 'Posts', index=False)\n",
        "\n",
        "    # Raport\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Scraping zakończony!\")\n",
        "    print(f\"Przetworzono artykułów: {len(all_results)}\")\n",
        "    print(f\"Błędów: {len(errors)}\")\n",
        "    if errors:\n",
        "        print(f\"\\nLinki z błędami (pierwsze 10):\")\n",
        "        for error_link in errors[:10]:\n",
        "            print(f\"  - {error_link}\")\n",
        "        if len(errors) > 10:\n",
        "            print(f\"  ... i {len(errors) - 10} więcej\")\n",
        "    print(f\"\\nPliki wyjściowe:\")\n",
        "    print(f\"  - glomb_{timestamp}.json\")\n",
        "    print(f\"  - glomb_{timestamp}.xlsx\")\n",
        "    print(f\"{'='*60}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epJ6xMAk12SA",
        "outputId": "f302ee86-95b6-4275-f52e-0c23f7f98034"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wczytano 149 linków z pliku\n",
            "\n",
            "============================================================\n",
            "Rozpoczynam scraping artykułów z jacekglomb.blogspot.com\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 149/149 [00:24<00:00,  6.07it/s]\n",
            "/tmp/ipython-input-3838759957.py:260: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  df.to_excel(writer, 'Posts', index=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Scraping zakończony!\n",
            "Przetworzono artykułów: 149\n",
            "Błędów: 0\n",
            "\n",
            "Pliki wyjściowe:\n",
            "  - glomb_2026-01-12.json\n",
            "  - glomb_2026-01-12.xlsx\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "BLKoiUJA1-KM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}