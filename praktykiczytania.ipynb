{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter pydrive"
      ],
      "metadata": {
        "id": "33hRz_rEJgHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c2c5cb-f6b6-41e0-80d6-1ec8d062282e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pydrive\n",
            "  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/987.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m983.0/987.4 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.12/dist-packages (from pydrive) (2.187.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.12/dist-packages (from pydrive) (6.0.3)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (0.31.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (2.43.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (0.2.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (2.28.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (4.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (4.9.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (1.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.72.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.2->pydrive) (6.2.4)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=1.2->pydrive) (3.2.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2025.11.12)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pydrive\n",
            "  Building wheel for pydrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pydrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27433 sha256=d3c6d156395be8b6df193715c2f6497cf3cdfe062ab387c37322332cc2fc35a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/10/da/a5b513f5b3916fc391c20ee7b4633e5cf3396d570cdd74970f\n",
            "Successfully built pydrive\n",
            "Installing collected packages: xlsxwriter, pydrive\n",
            "Successfully installed pydrive-1.3.1 xlsxwriter-3.2.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%% import\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "#%% functions\n",
        "\n",
        "def get_archive_page_links(archive_url):\n",
        "    \"\"\"\n",
        "    Pobiera linki do wszystkich miesięcy/lat z głównej strony archiwum\n",
        "    \"\"\"\n",
        "    try:\n",
        "        r = requests.get(archive_url)\n",
        "        r.encoding = 'utf-8'\n",
        "        soup = BeautifulSoup(r.text, 'lxml')\n",
        "        archive_links = []\n",
        "\n",
        "\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link['href']\n",
        "            if 'praktykiczytania.pl' in href and any(str(year) in href for year in range(2010, 2026)):\n",
        "                if href not in archive_links:\n",
        "                    archive_links.append(href)\n",
        "\n",
        "        return archive_links\n",
        "    except Exception as e:\n",
        "        print(f\"Błąd pobierania strony archiwum: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def get_article_links_from_month(month_url):\n",
        "    \"\"\"\n",
        "    Pobiera wszystkie linki do artykułów z danego miesiąca\n",
        "    \"\"\"\n",
        "    article_links = []\n",
        "    try:\n",
        "        r = requests.get(month_url)\n",
        "        r.encoding = 'utf-8'\n",
        "        soup = BeautifulSoup(r.text, 'lxml')\n",
        "\n",
        "        for article in soup.find_all('article'):\n",
        "            title_link = article.find('a', href=True)\n",
        "            if title_link and 'praktykiczytania.pl' in title_link['href']:\n",
        "                article_links.append(title_link['href'])\n",
        "\n",
        "        for heading in soup.find_all(['h2', 'h3'], class_=lambda x: x and ('entry-title' in x or 'post-title' in x)):\n",
        "            link = heading.find('a', href=True)\n",
        "            if link and 'praktykiczytania.pl' in link['href']:\n",
        "                article_links.append(link['href'])\n",
        "\n",
        "        content_area = soup.find('main') or soup.find('div', class_=lambda x: x and 'content' in x)\n",
        "        if content_area:\n",
        "            for link in content_area.find_all('a', href=True):\n",
        "                href = link['href']\n",
        "                if 'praktykiczytania.pl' in href and href not in article_links:\n",
        "                    if not any(x in href for x in ['/tag/', '/category/', '/author/', '/archiwum/', '/page/']):\n",
        "                        article_links.append(href)\n",
        "\n",
        "\n",
        "        seen = set()\n",
        "        unique_links = []\n",
        "        for link in article_links:\n",
        "            if link not in seen:\n",
        "                seen.add(link)\n",
        "                unique_links.append(link)\n",
        "\n",
        "        return unique_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Błąd pobierania artykułów z {month_url}: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def check_pagination(month_url):\n",
        "    \"\"\"\n",
        "    Sprawdza czy strona ma paginację i zwraca wszystkie strony\n",
        "    \"\"\"\n",
        "    pages = [month_url]\n",
        "    try:\n",
        "        r = requests.get(month_url)\n",
        "        r.encoding = 'utf-8'\n",
        "        soup = BeautifulSoup(r.text, 'lxml')\n",
        "\n",
        "        pagination = soup.find('nav', class_=lambda x: x and 'pagination' in x) or \\\n",
        "                    soup.find('div', class_=lambda x: x and 'pagination' in x)\n",
        "\n",
        "        if pagination:\n",
        "            for link in pagination.find_all('a', href=True):\n",
        "                page_url = link['href']\n",
        "                if page_url not in pages and 'praktykiczytania.pl' in page_url:\n",
        "                    pages.append(page_url)\n",
        "\n",
        "        next_link = soup.find('a', class_=lambda x: x and 'next' in x)\n",
        "        if next_link:\n",
        "            page_num = 2\n",
        "            while True:\n",
        "                next_page = f\"{month_url}page/{page_num}/\" if not month_url.endswith('/') else f\"{month_url}page/{page_num}/\"\n",
        "                r = requests.get(next_page)\n",
        "                if r.status_code == 200:\n",
        "                    pages.append(next_page)\n",
        "                    page_num += 1\n",
        "                    time.sleep(0.5)\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "        return pages\n",
        "    except:\n",
        "        return pages\n",
        "\n",
        "\n",
        "def get_all_article_links(archive_url):\n",
        "    \"\"\"\n",
        "    Główna funkcja - pobiera wszystkie linki do artykułów z całego archiwum\n",
        "    \"\"\"\n",
        "    print(\"Krok 1: Pobieranie listy miesięcy z archiwum...\")\n",
        "    archive_pages = get_archive_page_links(archive_url)\n",
        "\n",
        "    print(f\"Znaleziono {len(archive_pages)} stron archiwum\")\n",
        "\n",
        "    all_article_links = []\n",
        "    errors = []\n",
        "\n",
        "    print(\"\\nKrok 2: Pobieranie artykułów z każdego miesiąca...\")\n",
        "\n",
        "    for month_url in tqdm(archive_pages, desc=\"Przetwarzanie miesięcy\"):\n",
        "        try:\n",
        "            month_pages = check_pagination(month_url)\n",
        "\n",
        "            for page_url in month_pages:\n",
        "                time.sleep(0.5)\n",
        "                article_links = get_article_links_from_month(page_url)\n",
        "                all_article_links.extend(article_links)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nBłąd dla {month_url}: {e}\")\n",
        "            errors.append(month_url)\n",
        "    all_article_links = list(set(all_article_links))\n",
        "\n",
        "    return all_article_links, errors\n",
        "\n",
        "\n",
        "#%% main execution\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    archive_url = \"https://praktykiczytania.pl/archiwum/\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Pobieranie wszystkich linków z praktykiczytania.pl\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    article_links, errors = get_all_article_links(archive_url)\n",
        "\n",
        "    article_links.sort()\n",
        "    with open('praktykiczytania_linki.txt', 'w', encoding='utf-8') as f:\n",
        "        for link in article_links:\n",
        "            f.write(link + '\\n')\n",
        "\n",
        "    output_data = {\n",
        "        'source': archive_url,\n",
        "        'total_links': len(article_links),\n",
        "        'links': article_links,\n",
        "        'errors': errors\n",
        "    }\n",
        "\n",
        "    with open('praktykiczytania_linki.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Raport\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RAPORT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Znaleziono artykułów: {len(article_links)}\")\n",
        "    print(f\"Błędów: {len(errors)}\")\n",
        "    if errors:\n",
        "        print(f\"\\nProblematyczne URLe:\")\n",
        "        for error_url in errors:\n",
        "            print(f\"  - {error_url}\")\n",
        "    print(f\"\\nZapisano do:\")\n",
        "    print(f\"  - praktykiczytania_linki.txt\")\n",
        "    print(f\"  - praktykiczytania_linki.json\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "74X-Cr17Y-PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% import\n",
        "from __future__ import unicode_literals\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from time import mktime\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import json\n",
        "import xlsxwriter\n",
        "\n",
        "\n",
        "#%% functions\n",
        "\n",
        "def date_change_format(date_string):\n",
        "    \"\"\"\n",
        "    Konwertuje datę z różnych formatów na \"YYYY-MM-DD\"\n",
        "    Obsługuje formaty:\n",
        "    - \"09 marzec 2023\"\n",
        "    - \"2023-03-09\"\n",
        "    - \"09.03.2023\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Usuwamy dodatkowe białe znaki\n",
        "        date_string = ' '.join(date_string.strip().split())\n",
        "\n",
        "        # Jeśli już jest w formacie YYYY-MM-DD\n",
        "        if re.match(r'\\d{4}-\\d{2}-\\d{2}', date_string):\n",
        "            return date_string[:10]\n",
        "\n",
        "        # Jeśli jest datetime z czasem\n",
        "        if 'T' in date_string:\n",
        "            return date_string.split('T')[0]\n",
        "\n",
        "        # Słownik z obiema formami miesięcy\n",
        "        lookup_table = {\n",
        "            # Dopełniacz\n",
        "            \"stycznia\": \"01\", \"lutego\": \"02\", \"marca\": \"03\", \"kwietnia\": \"04\",\n",
        "            \"maja\": \"05\", \"czerwca\": \"06\", \"lipca\": \"07\", \"sierpnia\": \"08\",\n",
        "            \"września\": \"09\", \"października\": \"10\", \"listopada\": \"11\", \"grudnia\": \"12\",\n",
        "            # Mianownik\n",
        "            \"styczeń\": \"01\", \"luty\": \"02\", \"marzec\": \"03\", \"kwiecień\": \"04\",\n",
        "            \"maj\": \"05\", \"czerwiec\": \"06\", \"lipiec\": \"07\", \"sierpień\": \"08\",\n",
        "            \"wrzesień\": \"09\", \"październik\": \"10\", \"listopad\": \"11\", \"grudzień\": \"12\"\n",
        "        }\n",
        "\n",
        "        # Zamieniamy nazwę miesiąca na numer\n",
        "        for k, v in lookup_table.items():\n",
        "            date_string = date_string.replace(k, v)\n",
        "\n",
        "        # Format DD.MM.YYYY\n",
        "        if re.match(r'\\d{1,2}\\.\\d{1,2}\\.\\d{4}', date_string):\n",
        "            result = time.strptime(date_string, \"%d.%m.%Y\")\n",
        "        # Format DD MM YYYY\n",
        "        else:\n",
        "            result = time.strptime(date_string, \"%d %m %Y\")\n",
        "\n",
        "        changed_date = datetime.fromtimestamp(mktime(result))\n",
        "        new_date = format(changed_date.date())\n",
        "        return new_date\n",
        "    except Exception as e:\n",
        "        print(f\"Błąd konwersji daty '{date_string}': {e}\")\n",
        "        return \"no date\"\n",
        "\n",
        "\n",
        "def dictionary_of_article(article_link):\n",
        "    \"\"\"\n",
        "    Pobiera szczegóły artykułu ze strony praktykiczytania.pl\n",
        "    Zwraca dane w tym samym formacie co scraper dla jacekwakar.pl\n",
        "    \"\"\"\n",
        "    try:\n",
        "        r = requests.get(article_link)\n",
        "        r.encoding = 'utf-8'\n",
        "        html_text = r.text\n",
        "\n",
        "        # Obsługa rate limiting\n",
        "        while '429 Too Many Requests' in html_text:\n",
        "            time.sleep(5)\n",
        "            r = requests.get(article_link)\n",
        "            r.encoding = 'utf-8'\n",
        "            html_text = r.text\n",
        "\n",
        "        soup = BeautifulSoup(html_text, 'lxml')\n",
        "\n",
        "        # Data publikacji\n",
        "        try:\n",
        "            date_element = soup.find('time')\n",
        "            if date_element:\n",
        "                # Próbuj datetime attribute\n",
        "                date_text = date_element.get('datetime') or date_element.get_text(strip=True)\n",
        "                date_of_publication = date_change_format(date_text)\n",
        "            else:\n",
        "                # Alternatywnie szukaj w spanach/divach z \"date\"\n",
        "                date_element = soup.find(['span', 'div'], class_=lambda x: x and 'date' in str(x).lower())\n",
        "                if date_element:\n",
        "                    date_text = date_element.get_text(strip=True)\n",
        "                    date_of_publication = date_change_format(date_text)\n",
        "                else:\n",
        "                    date_of_publication = \"no date\"\n",
        "        except Exception as e:\n",
        "            print(f\"Błąd parsowania daty dla {article_link}: {e}\")\n",
        "            date_of_publication = \"no date\"\n",
        "\n",
        "        # Tytuł\n",
        "        try:\n",
        "            title_element = soup.find('h1')\n",
        "            title = title_element.get_text(strip=True) if title_element else \"no title\"\n",
        "        except:\n",
        "            title = \"no title\"\n",
        "\n",
        "        # Autor\n",
        "        try:\n",
        "            # Opcja 1: rel=\"author\"\n",
        "            author_element = soup.find('a', rel='author')\n",
        "            if not author_element:\n",
        "                # Opcja 2: klasa z \"author\"\n",
        "                author_element = soup.find(['span', 'div', 'a'], class_=lambda x: x and 'author' in str(x).lower())\n",
        "\n",
        "            if author_element:\n",
        "                author = author_element.get_text(strip=True)\n",
        "                # Usuwamy prefix \"Autor:\", \"By:\" itp.\n",
        "                author = re.sub(r'^(Autor|By|Opublikował|Posted by):\\s*', '', author, flags=re.IGNORECASE)\n",
        "            else:\n",
        "                author = \"no author\"\n",
        "        except:\n",
        "            author = \"no author\"\n",
        "\n",
        "        # Treść artykułu\n",
        "        try:\n",
        "            # Próbujemy różnych opcji\n",
        "            article_body = soup.find('div', class_=lambda x: x and 'entry-content' in str(x).lower())\n",
        "            if not article_body:\n",
        "                article_body = soup.find('div', class_=lambda x: x and 'post-content' in str(x).lower())\n",
        "            if not article_body:\n",
        "                article_body = soup.find('div', class_=lambda x: x and 'article-content' in str(x).lower())\n",
        "            if not article_body:\n",
        "                article_body = soup.find('article')\n",
        "\n",
        "            if article_body:\n",
        "                text = article_body.get_text(strip=True).replace('\\n', ' ').replace('\\xa0', ' ')\n",
        "            else:\n",
        "                text = \"no text\"\n",
        "        except:\n",
        "            text = \"no text\"\n",
        "\n",
        "        # Kategoria\n",
        "        try:\n",
        "            category_links = soup.find_all('a', rel='category')\n",
        "            if not category_links:\n",
        "                category_links = soup.find_all('a', class_=lambda x: x and 'category' in str(x).lower())\n",
        "\n",
        "            if category_links:\n",
        "                categories = [cat.get_text(strip=True) for cat in category_links]\n",
        "                category = ' | '.join(categories)\n",
        "            else:\n",
        "                category = \"no category\"\n",
        "        except:\n",
        "            category = \"no category\"\n",
        "\n",
        "        # Tagi\n",
        "        try:\n",
        "            tag_links = soup.find_all('a', rel='tag')\n",
        "            if not tag_links:\n",
        "                tag_links = soup.find_all('a', class_=lambda x: x and 'tag' in str(x).lower())\n",
        "\n",
        "            if tag_links:\n",
        "                tags = [tag.get_text(strip=True) for tag in tag_links]\n",
        "                tags_str = ' | '.join(tags)\n",
        "            else:\n",
        "                tags_str = None\n",
        "        except:\n",
        "            tags_str = None\n",
        "\n",
        "        # Linki zewnętrzne\n",
        "        try:\n",
        "            if article_body:\n",
        "                links = [a['href'] for a in article_body.find_all('a', href=True)]\n",
        "                # Filtrujemy linki wewnętrzne\n",
        "                external_links = [link for link in links if not re.search(r'praktykiczytania\\.pl', link)]\n",
        "                external_links = ' | '.join(external_links) if external_links else None\n",
        "            else:\n",
        "                external_links = None\n",
        "        except (AttributeError, KeyError, IndexError):\n",
        "            external_links = None\n",
        "\n",
        "        # Zdjęcia\n",
        "        try:\n",
        "            images = []\n",
        "\n",
        "            # 1. Thumbnail / post-thumbnail (główne zdjęcie artykułu)\n",
        "            thumbnail_div = soup.find('div', class_=lambda x: x and 'post-thumbnail' in str(x).lower())\n",
        "            if thumbnail_div:\n",
        "                thumb_img = thumbnail_div.find('img', src=True)\n",
        "                if thumb_img:\n",
        "                    images.append(thumb_img['src'])\n",
        "\n",
        "            # 2. Featured image (alternatywna nazwa)\n",
        "            if not images:\n",
        "                featured_img = soup.find('img', class_=lambda x: x and 'featured' in str(x).lower())\n",
        "                if not featured_img:\n",
        "                    featured_div = soup.find('div', class_=lambda x: x and 'featured' in str(x).lower())\n",
        "                    if featured_div:\n",
        "                        featured_img = featured_div.find('img')\n",
        "\n",
        "                if featured_img and featured_img.get('src'):\n",
        "                    if featured_img['src'] not in images:\n",
        "                        images.append(featured_img['src'])\n",
        "\n",
        "            # 3. Zdjęcia w treści artykułu\n",
        "            if article_body:\n",
        "                content_images = [img['src'] for img in article_body.find_all('img', src=True)]\n",
        "                for img_src in content_images:\n",
        "                    if img_src not in images:\n",
        "                        images.append(img_src)\n",
        "\n",
        "            # 4. Inne możliwe miejsca na obrazy (header, figure, etc.)\n",
        "            for container_class in ['entry-header', 'article-header', 'post-header']:\n",
        "                header = soup.find('div', class_=container_class)\n",
        "                if header:\n",
        "                    header_images = [img['src'] for img in header.find_all('img', src=True)]\n",
        "                    for img_src in header_images:\n",
        "                        if img_src not in images:\n",
        "                            images.append(img_src)\n",
        "\n",
        "            has_images = len(images) > 0\n",
        "            photos_links = ' | '.join(images) if images else None\n",
        "        except (AttributeError, KeyError, IndexError):\n",
        "            has_images = False\n",
        "            photos_links = None\n",
        "\n",
        "        # Filmy (iframe)\n",
        "        try:\n",
        "            if article_body:\n",
        "                iframes = [iframe['src'] for iframe in article_body.find_all('iframe', src=True)]\n",
        "                has_videos = len(iframes) > 0\n",
        "            else:\n",
        "                has_videos = False\n",
        "        except:\n",
        "            has_videos = False\n",
        "\n",
        "        dictionary_of_article = {\n",
        "            \"Link\": article_link,\n",
        "            \"Data publikacji\": date_of_publication,\n",
        "            \"Tytuł artykułu\": title.replace('\\xa0', ' '),\n",
        "            \"Tekst artykułu\": text,\n",
        "            \"Autor\": author,\n",
        "            \"Kategoria\": category,\n",
        "            \"Tagi\": tags_str,  # Dodane pole - nie było w jacekwakar.pl\n",
        "            \"Linki zewnętrzne\": external_links,\n",
        "            \"Zdjęcia/Grafika\": has_images,\n",
        "            \"Filmy\": has_videos,\n",
        "            \"Linki do zdjęć\": photos_links\n",
        "        }\n",
        "\n",
        "        all_results.append(dictionary_of_article)\n",
        "\n",
        "    except AttributeError as e:\n",
        "        errors.append(article_link)\n",
        "        print(f\"Błąd dla {article_link}: {e}\")\n",
        "    except Exception as e:\n",
        "        errors.append(article_link)\n",
        "        print(f\"Nieoczekiwany błąd dla {article_link}: {e}\")\n",
        "\n",
        "\n",
        "#%% main execution\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Wczytaj linki z pliku\n",
        "    try:\n",
        "        with open('praktykiczytania_linki.txt', 'r', encoding='utf-8') as f:\n",
        "            article_links = [line.strip() for line in f if line.strip()]\n",
        "        print(f\"Wczytano {len(article_links)} linków z pliku\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Nie znaleziono pliku praktykiczytania_linki.txt\")\n",
        "        print(\"Użyj najpierw get_praktyki_links.py aby pobrać linki!\")\n",
        "        print(\"\\nLub podaj linki ręcznie:\")\n",
        "        article_links = [\n",
        "            # Wstaw tutaj linki do artykułów\n",
        "            # \"https://praktykiczytania.pl/artykul1/\",\n",
        "            # \"https://praktykiczytania.pl/artykul2/\",\n",
        "        ]\n",
        "\n",
        "    if not article_links:\n",
        "        print(\"Brak linków do przetworzenia!\")\n",
        "        exit(1)\n",
        "\n",
        "    all_results = []\n",
        "    errors = []\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Rozpoczynam scraping artykułów z praktykiczytania.pl\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Scraping z progress barem\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        list(tqdm(executor.map(dictionary_of_article, article_links), total=len(article_links)))\n",
        "\n",
        "    # Zapisywanie wyników\n",
        "    timestamp = datetime.today().date()\n",
        "\n",
        "    # JSON\n",
        "    with open(f'praktykiczytania_{timestamp}.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)\n",
        "\n",
        "    # Excel\n",
        "    df = pd.DataFrame(all_results)\n",
        "    with pd.ExcelWriter(f\"praktykiczytania_{timestamp}.xlsx\",\n",
        "                       engine='xlsxwriter',\n",
        "                       engine_kwargs={'options': {'strings_to_urls': False}}) as writer:\n",
        "        df.to_excel(writer, 'Posts', index=False)\n",
        "\n",
        "    # Raport\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Scraping zakończony!\")\n",
        "    print(f\"Przetworzono artykułów: {len(all_results)}\")\n",
        "    print(f\"Błędów: {len(errors)}\")\n",
        "    if errors:\n",
        "        print(f\"\\nLinki z błędami (pierwsze 10):\")\n",
        "        for error_link in errors[:10]:\n",
        "            print(f\"  - {error_link}\")\n",
        "        if len(errors) > 10:\n",
        "            print(f\"  ... i {len(errors) - 10} więcej\")\n",
        "    print(f\"\\nPliki wyjściowe:\")\n",
        "    print(f\"  - praktykiczytania_{timestamp}.json\")\n",
        "    print(f\"  - praktykiczytania_{timestamp}.xlsx\")\n",
        "    print(f\"{'='*60}\\n\")"
      ],
      "metadata": {
        "id": "86WotCDzYSIz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}