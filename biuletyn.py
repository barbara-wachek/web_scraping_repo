# -*- coding: utf-8 -*-
"""biuletyn

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m-LqjRZaFLl3hzrvIIKUcW2wm4xW2qkH
"""

#%% import
from __future__ import unicode_literals
import regex as re
import time
from datetime import datetime
from time import mktime
import requests
from bs4 import BeautifulSoup
from __future__ import unicode_literals
import requests
from bs4 import BeautifulSoup
import pandas as pd
import regex as re
import time
from time import mktime
from tqdm import tqdm  #licznik
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
import json
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
import xlsxwriter

url_articles = 'https://biuletynpolonistyczny.pl/pl/articles/?csrfmiddlewaretoken=cwbzN7DO98u6HQpqTp4aEYtGB9vvvwBd&article_title_text=&article_contributors=&o=article_title_text&per_page=200'
url_events = "https://biuletynpolonistyczny.pl/pl/events/?csrfmiddlewaretoken=cwbzN7DO98u6HQpqTp4aEYtGB9vvvwBd&event_institutions=&event_name_text=&event_city__country=&event_city=&event_date_from=&event_date_to=&event_contributors_date=&event_participants_date=&event_status=FURTHCOMING&event_status=IN_PROGRESS&event_status=PAST&o=event_date_from%2Cevent_time_from&per_page=4000"

#ARTICLES


response = requests.get(url_articles)
soup = BeautifulSoup(response.text, 'html.parser')
nazwa_pliku = 'biuletyn_articles.txt'
with open(nazwa_pliku, 'w', encoding='utf-8') as plik:
    plik.write(response.text)

all_links = []
article_links = []
for link in soup.find_all('a'):
    all_links.append(link.get('href'))

for link in all_links:
  try:
    if "article_title_text" in link:
      article_links.append(link)
  except:
    pass

def dictionary_of_event(article_link):


    base_link = "https://biuletynpolonistyczny.pl"
    r = requests.get(base_link + article_link)
    html_text = r.text
    while '429 Too Many Requests' in html_text:
        time.sleep(10)
        html_text = requests.get(base_link + article_link).text
    soup = BeautifulSoup(html_text, 'html.parser')
    try:
      date_of_publication = soup.find('span', class_='details__text--date').text
    except:
      date_of_publication = "no date"
    try:
      text_of_article = soup.find('div', class_='details__explication').text.strip().replace('\n', ' ').replace('\xa0', ' ')
    except:
      text_of_article = "no text"
    try:
      autor = soup.find('a', class_='details__text--value details__text--anchor details__text--bold').text
    except:
      autor = None
    try:
      title_of_article = soup.find('h1', class_='details__text--title').text.strip()
    except:
      title_of_article = None
    try:
      tags = '|'.join(set([e.text for e in soup.find_all('a', class_='details__text--anchor-border details__text--value')]))
    except:
      tags = None

    try:
        external_links = '|'.join(set([e.text for e in soup.find_all('a', class_='details__text--anchor') if 'http' in e.get('href', '')]))
    except (AttributeError, KeyError, IndexError):
        external_links = None

    try:
        photos_links = '|'.join([img.get('src') for img in soup.find_all('img') if img.get('src')])
    except (AttributeError, KeyError, IndexError):
        photos_links = None

    try:
        videos_links = '|'.join([img.get('src') for img in soup.find_all('iframe') if img.get('src')])
    except (AttributeError, KeyError, IndexError):
        videos_links = None

    dictionary_of_article = {"Link": base_link + article_link,
                              "Data publikacji": date_of_publication,
                              "Tytuł artykułu": title_of_article.replace('\xa0', ' '),
                              "Tekst artykułu": text_of_article,
                              "Autor": autor,
                              "Tagi": tags,
                              "Kategoria": "artykuł",
                              'Linki zewnętrzne': external_links,
                              'Zdjęcia/Grafika': True if photos_links else False,
                              'Filmy': True if videos_links else False,
                              'Linki do zdjęć': photos_links
                              }

    all_results.append(dictionary_of_article)

#First 8 links are just URLs for search results
article_links = article_links[8:]

all_results = []
errors = []
with ThreadPoolExecutor() as excecutor:
    list(tqdm(excecutor.map(dictionary_of_event, article_links),total=len(article_links)))
with open(f'biuletyn_articles{datetime.today().date()}.json', 'w', encoding='utf-8') as f:
    json.dump(all_results, f, ensure_ascii=False, default=str)
df = pd.DataFrame(all_results)
df["Data publikacji"] = pd.to_datetime(df["Data publikacji"]).dt.date
df = df.sort_values('Data publikacji', ascending=False)
with pd.ExcelWriter(f"biuletyn_polonistyczny_articles{datetime.today().date()}.xlsx", engine='xlsxwriter', engine_kwargs={'options': {'strings_to_urls': False}}) as writer:
    df.to_excel(writer, 'Posts', index=False)

#EVENTS


response = requests.get(url_events)
soup = BeautifulSoup(response.text, 'html.parser')
nazwa_pliku = 'biuletyn_events.txt'
with open(nazwa_pliku, 'w', encoding='utf-8') as plik:
    plik.write(response.text)

all_links = []
article_links = []
for link in soup.find_all('a'):
    all_links.append(link.get('href'))

for link in all_links:
  try:
    if "event_name_text" in link:
      article_links.append(link)
  except:
    pass

def dictionary_of_event(article_link):


    base_link = "https://biuletynpolonistyczny.pl"
    r = requests.get(base_link + article_link)
    html_text = r.text
    while '429 Too Many Requests' in html_text:
        time.sleep(10)
        html_text = requests.get(base_link + article_link).text
    soup = BeautifulSoup(html_text, 'html.parser')
    try:
      date_of_publication = soup.find('span', class_='details__text--date').text
    except:
      date_of_publication = "no date"
    try:
      text_of_article = soup.find('div', class_='details__explication').text.strip().replace('\n', ' ').replace('\xa0', ' ')
    except:
      text_of_article = "no text"
    try:
      autor = soup.find('a', class_='details__text--value details__text--anchor details__text--bold').text
    except:
      autor = None
    try:
      title_of_article = soup.find('h1', class_='details__text--title').text.strip()
    except:
      title_of_article = None
    try:
      tags = '|'.join(set([e.text for e in soup.find_all('a', class_='details__text--anchor-border details__text--value')]))
    except:
      tags = None

    try:
        external_links = '|'.join(set([e.text for e in soup.find_all('a', class_='details__text--anchor') if 'http' in e.get('href', '')]))
    except (AttributeError, KeyError, IndexError):
        external_links = None

    try:
        photos_links = '|'.join([img.get('src') for img in soup.find_all('img') if img.get('src')])
    except (AttributeError, KeyError, IndexError):
        photos_links = None

    try:
        videos_links = '|'.join([img.get('src') for img in soup.find_all('iframe') if img.get('src')])
    except (AttributeError, KeyError, IndexError):
        videos_links = None

    dictionary_of_article = {"Link": base_link + article_link,
                              "Data publikacji": date_of_publication,
                              "Tytuł artykułu": title_of_article.replace('\xa0', ' '),
                              "Tekst artykułu": text_of_article,
                              "Autor": autor,
                              "Tagi": tags,
                              "Kategoria": "wydarzenie",
                              'Linki zewnętrzne': external_links,
                              'Zdjęcia/Grafika': True if photos_links else False,
                              'Filmy': True if videos_links else False,
                              'Linki do zdjęć': photos_links
                              }

    all_results.append(dictionary_of_article)

#First 8 links are just URLs for search results
article_links = article_links[8:]

all_results = []
errors = []
with ThreadPoolExecutor() as excecutor:
    list(tqdm(excecutor.map(dictionary_of_event, article_links),total=len(article_links)))
with open(f'biuletyn_events{datetime.today().date()}.json', 'w', encoding='utf-8') as f:
    json.dump(all_results, f, ensure_ascii=False, default=str)
df = pd.DataFrame(all_results)
with pd.ExcelWriter(f"biuletyn_polonistyczny_events{datetime.today().date()}.xlsx", engine='xlsxwriter', engine_kwargs={'options': {'strings_to_urls': False}}) as writer:
    df.to_excel(writer, 'Posts', index=False)