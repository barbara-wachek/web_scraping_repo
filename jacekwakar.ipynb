{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter pydrive"
      ],
      "metadata": {
        "id": "33hRz_rEJgHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_all_links(first_article_link, base_url=None, visited=None):\n",
        "    \"\"\"\n",
        "    Rekurencyjnie pobiera linki do wszystkich artykułów,\n",
        "    przechodząc przez kolejne strony oznaczone przyciskiem 'Następny artykuł'.\n",
        "\n",
        "    Args:\n",
        "        first_article_link (str): pełny lub względny URL pierwszego artykułu.\n",
        "        base_url (str, optional): adres bazowy strony (np. 'https://example.com').\n",
        "        visited (set, optional): zbiór odwiedzonych linków (dla uniknięcia pętli).\n",
        "\n",
        "    Returns:\n",
        "        list[str]: lista wszystkich linków do artykułów w kolejności ich występowania.\n",
        "    \"\"\"\n",
        "    if visited is None:\n",
        "        visited = set()\n",
        "    if base_url is None:\n",
        "\n",
        "        from urllib.parse import urljoin\n",
        "        base_url = first_article_link.split('/blog/')[0]\n",
        "\n",
        "\n",
        "    from urllib.parse import urljoin\n",
        "    full_url = urljoin(base_url, first_article_link)\n",
        "\n",
        "\n",
        "    if full_url in visited:\n",
        "        return []\n",
        "\n",
        "    visited.add(full_url)\n",
        "    print(f\"Pobieram: {full_url}\")\n",
        "\n",
        "\n",
        "    response = requests.get(full_url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Błąd {response.status_code} przy pobieraniu {full_url}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "\n",
        "    next_link = soup.find(\"a\", class_=\"next\")\n",
        "\n",
        "\n",
        "    if next_link and next_link.get(\"href\"):\n",
        "        next_href = next_link[\"href\"]\n",
        "        return [full_url] + get_all_links(next_href, base_url, visited)\n",
        "    else:\n",
        "\n",
        "        return [full_url]\n"
      ],
      "metadata": {
        "id": "74X-Cr17Y-PE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links = get_all_links(\"https://jacekwakar.pl/blog/varia/antygona-z-bejrutu\")\n",
        "links_recenzje = get_all_links(\"https://jacekwakar.pl/blog/recenzje/krzyk-we-mnie-wielki-wzbiera\")\n",
        "links_eseje = get_all_links(\"https://jacekwakar.pl/blog/eseje/bardzo-dobrze-dostateczny\")\n",
        "links_rozmowy = get_all_links(\"https://jacekwakar.pl/blog/rozmowy/nie-bedziemy-pisac-manifestow\")\n",
        "links_sylwetki = get_all_links(\"https://jacekwakar.pl/blog/sylwetki/blisko-coraz-blizej\")\n",
        "all_links = links + links_recenzje + links_eseje + links_rozmowy + links_sylwetki"
      ],
      "metadata": {
        "id": "3rKAkqMGY_55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% import\n",
        "from __future__ import unicode_literals\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from time import mktime\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm  #licznik\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import json\n",
        "import xlsxwriter\n",
        "\n",
        "\n",
        "#%% functions\n",
        "\n",
        "def date_change_format(date_string):\n",
        "    \"\"\"\n",
        "    Konwertuje datę z formatu \"09 marzec 2023\" na \"2023-03-09\"\n",
        "    Obsługuje obie formy miesięcy (mianownik i dopełniacz)\n",
        "    \"\"\"\n",
        "    try:\n",
        "\n",
        "        date_string = ' '.join(date_string.strip().split())\n",
        "\n",
        "\n",
        "        lookup_table = {\n",
        "\n",
        "            \"stycznia\": \"01\", \"lutego\": \"02\", \"marca\": \"03\", \"kwietnia\": \"04\",\n",
        "            \"maja\": \"05\", \"czerwca\": \"06\", \"lipca\": \"07\", \"sierpnia\": \"08\",\n",
        "            \"września\": \"09\", \"października\": \"10\", \"listopada\": \"11\", \"grudnia\": \"12\",\n",
        "\n",
        "            \"styczeń\": \"01\", \"luty\": \"02\", \"marzec\": \"03\", \"kwiecień\": \"04\",\n",
        "            \"maj\": \"05\", \"czerwiec\": \"06\", \"lipiec\": \"07\", \"sierpień\": \"08\",\n",
        "            \"wrzesień\": \"09\", \"październik\": \"10\", \"listopad\": \"11\", \"grudzień\": \"12\"\n",
        "        }\n",
        "\n",
        "\n",
        "        for k, v in lookup_table.items():\n",
        "            date_string = date_string.replace(k, v)\n",
        "\n",
        "\n",
        "        result = time.strptime(date_string, \"%d %m %Y\")\n",
        "        changed_date = datetime.fromtimestamp(mktime(result))\n",
        "        new_date = format(changed_date.date())\n",
        "        return new_date\n",
        "    except Exception as e:\n",
        "        print(f\"Błąd konwersji daty '{date_string}': {e}\")\n",
        "        return \"no date\"\n",
        "\n",
        "\n",
        "def dictionary_of_article(article_link):\n",
        "    \"\"\"\n",
        "    Pobiera szczegóły artykułu ze strony jacekwakar.pl\n",
        "    \"\"\"\n",
        "    try:\n",
        "        r = requests.get(article_link)\n",
        "\n",
        "        r.encoding = 'utf-8'\n",
        "        html_text = r.text\n",
        "\n",
        "\n",
        "        while '429 Too Many Requests' in html_text:\n",
        "            time.sleep(5)\n",
        "            r = requests.get(article_link)\n",
        "            r.encoding = 'utf-8'\n",
        "            html_text = r.text\n",
        "\n",
        "        soup = BeautifulSoup(html_text, 'lxml')\n",
        "\n",
        "\n",
        "        try:\n",
        "\n",
        "            published_span = soup.find('span', class_='published')\n",
        "            if published_span:\n",
        "                date_element = published_span.find('time', itemprop='datePublished')\n",
        "                if date_element:\n",
        "                    date_text = date_element.text.strip()\n",
        "                    date_of_publication = date_change_format(date_text)\n",
        "                else:\n",
        "                    date_of_publication = \"no date\"\n",
        "            else:\n",
        "                date_of_publication = \"no date\"\n",
        "        except Exception as e:\n",
        "            print(f\"Błąd parsowania daty: {e}\")\n",
        "            date_of_publication = \"no date\"\n",
        "\n",
        "\n",
        "        try:\n",
        "            title_element = soup.find('h1', itemprop='headline')\n",
        "            if not title_element:\n",
        "                title_element = soup.find('h1', class_='entry-title')\n",
        "            title = title_element.text.strip() if title_element else \"no title\"\n",
        "        except:\n",
        "            title = \"no title\"\n",
        "\n",
        "\n",
        "        try:\n",
        "            author_element = soup.find('span', class_='createdby')\n",
        "            if author_element:\n",
        "                author_name = author_element.find(itemprop='name')\n",
        "                author = author_name.text.strip() if author_name else \"no author\"\n",
        "            else:\n",
        "                author = \"no author\"\n",
        "        except:\n",
        "            author = \"no author\"\n",
        "\n",
        "\n",
        "        try:\n",
        "            article_body = soup.find('div', itemprop='articleBody')\n",
        "            if article_body:\n",
        "                text = article_body.text.strip().replace('\\n', ' ').replace('\\xa0', ' ')\n",
        "            else:\n",
        "                text = \"no text\"\n",
        "        except:\n",
        "            text = \"no text\"\n",
        "\n",
        "\n",
        "        try:\n",
        "            category_element = soup.find('span', class_='category-name')\n",
        "            if category_element:\n",
        "                category_link = category_element.find('a')\n",
        "                category = category_link.text.strip() if category_link else \"no category\"\n",
        "            else:\n",
        "                category = \"no category\"\n",
        "        except:\n",
        "            category = \"no category\"\n",
        "\n",
        "\n",
        "        try:\n",
        "            if article_body:\n",
        "                links = [a['href'] for a in article_body.find_all('a', href=True)]\n",
        "\n",
        "                external_links = [link for link in links if not re.search(r'jacekwakar\\.pl', link)]\n",
        "                external_links = ' | '.join(external_links) if external_links else None\n",
        "            else:\n",
        "                external_links = None\n",
        "        except (AttributeError, KeyError, IndexError):\n",
        "            external_links = None\n",
        "\n",
        "\n",
        "        try:\n",
        "            if article_body:\n",
        "                images = [img['src'] for img in article_body.find_all('img', src=True)]\n",
        "                has_images = len(images) > 0\n",
        "            else:\n",
        "                images = []\n",
        "                has_images = False\n",
        "        except (AttributeError, KeyError, IndexError):\n",
        "            images = []\n",
        "            has_images = False\n",
        "\n",
        "\n",
        "        try:\n",
        "            main_image = soup.find('div', class_='article-full-image')\n",
        "            if main_image:\n",
        "                main_img = main_image.find('img')\n",
        "                if main_img and 'src' in main_img.attrs:\n",
        "                    main_image_link = main_img['src']\n",
        "\n",
        "                    if main_image_link not in images:\n",
        "                        images.insert(0, main_image_link)\n",
        "                    has_images = True\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        photos_links = ' | '.join(images) if images else None\n",
        "\n",
        "\n",
        "        try:\n",
        "            if article_body:\n",
        "                iframes = [iframe['src'] for iframe in article_body.find_all('iframe', src=True)]\n",
        "                has_videos = len(iframes) > 0\n",
        "            else:\n",
        "                has_videos = False\n",
        "        except:\n",
        "            has_videos = False\n",
        "\n",
        "        dictionary_of_article = {\n",
        "            \"Link\": article_link,\n",
        "            \"Data publikacji\": date_of_publication,\n",
        "            \"Tytuł artykułu\": title.replace('\\xa0', ' '),\n",
        "            \"Tekst artykułu\": text,\n",
        "            \"Autor\": author,\n",
        "            \"Kategoria\": category,\n",
        "            \"Linki zewnętrzne\": external_links,\n",
        "            \"Zdjęcia/Grafika\": has_images,\n",
        "            \"Filmy\": has_videos,\n",
        "            \"Linki do zdjęć\": photos_links\n",
        "        }\n",
        "\n",
        "        all_results.append(dictionary_of_article)\n",
        "\n",
        "    except AttributeError as e:\n",
        "        errors.append(article_link)\n",
        "        print(f\"Błąd dla {article_link}: {e}\")\n",
        "    except Exception as e:\n",
        "        errors.append(article_link)\n",
        "        print(f\"Nieoczekiwany błąd dla {article_link}: {e}\")\n",
        "\n",
        "\n",
        "#%% main execution\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    article_links = all_links\n",
        "    all_results = []\n",
        "    errors = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        list(tqdm(executor.map(dictionary_of_article, article_links), total=len(article_links)))\n",
        "\n",
        "    timestamp = datetime.today().date()\n",
        "\n",
        "    with open(f'jacekwakar_{timestamp}.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)\n",
        "\n",
        "    df = pd.DataFrame(all_results)\n",
        "    with pd.ExcelWriter(f\"jacekwakar_{timestamp}.xlsx\",\n",
        "                       engine='xlsxwriter',\n",
        "                       engine_kwargs={'options': {'strings_to_urls': False}}) as writer:\n",
        "        df.to_excel(writer, 'Posts', index=False)\n",
        "\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Scraping zakończony!\")\n",
        "    print(f\"Przetworzono artykułów: {len(all_results)}\")\n",
        "    print(f\"Błędów: {len(errors)}\")\n",
        "    if errors:\n",
        "        print(f\"\\nLinki z błędami:\")\n",
        "        for error_link in errors:\n",
        "            print(f\"  - {error_link}\")\n",
        "    print(f\"{'='*50}\\n\")"
      ],
      "metadata": {
        "id": "1e92erxAKdor"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}