{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter pydrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CCminLFmG75",
        "outputId": "bda0876b-9743-4f7c-c71b-379ae8875a47"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pydrive\n",
            "  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.12/dist-packages (from pydrive) (2.187.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.12/dist-packages (from pydrive) (6.0.3)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (0.31.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (2.43.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (0.3.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (2.29.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (4.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (4.9.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (1.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.72.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.27.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.2->pydrive) (6.2.4)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=1.2->pydrive) (3.3.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2026.1.4)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pydrive\n",
            "  Building wheel for pydrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pydrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27433 sha256=cd1076bba3a1e8f331071be67ae058f443c4afdf6d2e14bd195d88b5b28dddd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/10/da/a5b513f5b3916fc391c20ee7b4633e5cf3396d570cdd74970f\n",
            "Successfully built pydrive\n",
            "Installing collected packages: xlsxwriter, pydrive\n",
            "Successfully installed pydrive-1.3.1 xlsxwriter-3.2.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ6GLDsPmEYV",
        "outputId": "4f15506f-491e-4c73-8745-b056c0f62706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Pobieranie linków z bukbuk.pl\n",
            "============================================================\n",
            "\n",
            "Krok 1: Próba pobrania sitemap...\n",
            "  Próbuję: https://bukbuk.pl/post-sitemap.xml\n",
            "  ✓ Znaleziono sitemap: https://bukbuk.pl/post-sitemap.xml\n",
            "  ✓ Linków: 757\n",
            "Znaleziono 757 linków w sitemap\n",
            "\n",
            "Znaleziono 757 linków\n",
            "\n",
            "Statystyki filtrowania:\n",
            "  Kategorie: 0\n",
            "  Tagi: 0\n",
            "  Autorzy: 0\n",
            "  Paginacja: 0\n",
            "  Same cyfry: 0\n",
            "  Za krótkie: 0\n",
            "  Media/pliki: 549\n",
            "  Inne: 1\n",
            "  ✅ ZAAKCEPTOWANO: 207\n",
            "\n",
            "Przykładowe linki (pierwsze 10):\n",
            "  1. https://bukbuk.pl/2016/02/bialystok-biala-sila\n",
            "  2. https://bukbuk.pl/2016/02/bukbuk-live\n",
            "  3. https://bukbuk.pl/2016/02/jak-napisac-kryminal\n",
            "  4. https://bukbuk.pl/2016/02/kazik-staszewski-o-swojej-biografii\n",
            "  5. https://bukbuk.pl/2016/02/kazik-staszewski-poleca\n",
            "  6. https://bukbuk.pl/2016/02/kinga-debska-poleca-ksiazki\n",
            "  7. https://bukbuk.pl/2016/02/kuba-zulczyk-o-bialym-proszku-i-swiatlach-wielkiego-miasta\n",
            "  8. https://bukbuk.pl/2016/02/magda-molek-poleca-ksiazki\n",
            "  9. https://bukbuk.pl/2016/02/malgorzata-halber-i-bohater\n",
            "  10. https://bukbuk.pl/2016/02/malgorzata-halber-poleca\n",
            "  ... i 197 więcej\n",
            "\n",
            "============================================================\n",
            "RAPORT\n",
            "============================================================\n",
            "Znaleziono artykułów: 207\n",
            "Błędów: 0\n",
            "\n",
            "Zapisano do:\n",
            "  - bukbuk_linki.txt\n",
            "  - bukbuk_linki.json\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "#%% import\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "#%% functions\n",
        "\n",
        "def try_sitemap(base_url):\n",
        "    \"\"\"\n",
        "    Próbuje pobrać linki z sitemap.xml\n",
        "    \"\"\"\n",
        "    sitemap_urls = [\n",
        "        f\"{base_url}post-sitemap.xml\",  # Priorytet dla bukbuk.pl\n",
        "        f\"{base_url}sitemap.xml\",\n",
        "        f\"{base_url}sitemap_index.xml\",\n",
        "        f\"{base_url}sitemap-posts.xml\",\n",
        "        f\"{base_url}wp-sitemap.xml\"\n",
        "    ]\n",
        "\n",
        "    for sitemap_url in sitemap_urls:\n",
        "        try:\n",
        "            print(f\"  Próbuję: {sitemap_url}\")\n",
        "            r = requests.get(sitemap_url, timeout=10)\n",
        "            if r.status_code == 200:\n",
        "                soup = BeautifulSoup(r.text, 'xml')\n",
        "                links = [loc.text.strip() for loc in soup.find_all('loc')]\n",
        "                if links:\n",
        "                    print(f\"  ✓ Znaleziono sitemap: {sitemap_url}\")\n",
        "                    print(f\"  ✓ Linków: {len(links)}\")\n",
        "                    return links\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_article_links_from_page(page_url):\n",
        "    \"\"\"\n",
        "    Pobiera linki do artykułów z pojedynczej strony\n",
        "    \"\"\"\n",
        "    try:\n",
        "        r = requests.get(page_url, timeout=10)\n",
        "        r.encoding = 'utf-8'\n",
        "        soup = BeautifulSoup(r.text, 'lxml')\n",
        "\n",
        "        article_links = []\n",
        "\n",
        "        # Szukamy w <article>\n",
        "        for article in soup.find_all('article'):\n",
        "            for header in article.find_all(['h1', 'h2', 'h3']):\n",
        "                link = header.find('a', href=True)\n",
        "                if link:\n",
        "                    href = link['href']\n",
        "                    if 'bukbuk.pl' in href or href.startswith('/'):\n",
        "                        if href.startswith('/'):\n",
        "                            href = 'https://bukbuk.pl' + href\n",
        "                        article_links.append(href)\n",
        "                        break\n",
        "\n",
        "        return article_links\n",
        "    except Exception as e:\n",
        "        print(f\"Błąd dla {page_url}: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def get_all_pages(base_url):\n",
        "    \"\"\"\n",
        "    Znajduje wszystkie strony paginacji\n",
        "    \"\"\"\n",
        "    pages = [base_url]\n",
        "    try:\n",
        "        page_num = 2\n",
        "        while page_num <= 100:  # Max 100 stron\n",
        "            # Różne formaty paginacji\n",
        "            possible_urls = [\n",
        "                f\"{base_url}page/{page_num}/\",\n",
        "                f\"{base_url}strona/{page_num}/\",\n",
        "                f\"{base_url}?page={page_num}\",\n",
        "                f\"{base_url}?paged={page_num}\"\n",
        "            ]\n",
        "\n",
        "            found = False\n",
        "            for next_page in possible_urls:\n",
        "                try:\n",
        "                    r = requests.get(next_page, timeout=10)\n",
        "                    if r.status_code == 200 and r.url != base_url:\n",
        "                        pages.append(next_page)\n",
        "                        print(f\"  Znaleziono stronę {page_num}\")\n",
        "                        found = True\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if not found:\n",
        "                break\n",
        "\n",
        "            page_num += 1\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        return pages\n",
        "    except Exception as e:\n",
        "        print(f\"Błąd paginacji: {e}\")\n",
        "        return pages\n",
        "\n",
        "\n",
        "def get_all_article_links(base_url):\n",
        "    \"\"\"\n",
        "    Główna funkcja - pobiera wszystkie linki\n",
        "    \"\"\"\n",
        "    # Najpierw próbuj sitemap\n",
        "    print(\"Krok 1: Próba pobrania sitemap...\")\n",
        "    sitemap_links = try_sitemap(base_url)\n",
        "\n",
        "    if sitemap_links:\n",
        "        print(f\"Znaleziono {len(sitemap_links)} linków w sitemap\")\n",
        "        return sitemap_links, []\n",
        "\n",
        "    print(\"Brak sitemap, używam paginacji...\")\n",
        "\n",
        "    # Paginacja\n",
        "    print(\"\\nKrok 2: Szukanie stron...\")\n",
        "    all_pages = get_all_pages(base_url)\n",
        "    print(f\"Znaleziono {len(all_pages)} stron\")\n",
        "\n",
        "    all_article_links = []\n",
        "    errors = []\n",
        "\n",
        "    print(\"\\nKrok 3: Pobieranie artykułów...\")\n",
        "    for page_url in tqdm(all_pages, desc=\"Przetwarzanie\"):\n",
        "        try:\n",
        "            time.sleep(0.5)\n",
        "            article_links = get_article_links_from_page(page_url)\n",
        "            all_article_links.extend(article_links)\n",
        "        except Exception as e:\n",
        "            errors.append(page_url)\n",
        "\n",
        "    # Usuń duplikaty\n",
        "    all_article_links = list(set(all_article_links))\n",
        "\n",
        "    return all_article_links, errors\n",
        "\n",
        "\n",
        "def filter_article_links(all_links):\n",
        "    \"\"\"\n",
        "    Filtruje linki - zostawia tylko artykuły\n",
        "    \"\"\"\n",
        "    article_links = []\n",
        "    excluded = {'category': 0, 'tag': 0, 'page': 0, 'author': 0, 'short': 0, 'digits': 0, 'media': 0, 'other': 0}\n",
        "\n",
        "    for link in all_links:\n",
        "        # Usuń fragmenty\n",
        "        link = link.split('#')[0].rstrip('/')\n",
        "\n",
        "        if not link:\n",
        "            continue\n",
        "\n",
        "        # Wykluczamy pliki multimedialne i statyczne\n",
        "        media_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg',\n",
        "                          '.pdf', '.zip', '.mp4', '.mp3', '.avi',\n",
        "                          '.css', '.js', '.xml', '.json']\n",
        "        if any(link.lower().endswith(ext) for ext in media_extensions):\n",
        "            excluded['media'] += 1\n",
        "            continue\n",
        "\n",
        "        # Wykluczamy ścieżki do folderów z plikami\n",
        "        if '/wp-content/' in link or '/uploads/' in link or '/files/' in link:\n",
        "            excluded['media'] += 1\n",
        "            continue\n",
        "\n",
        "        # Wykluczamy kategorie\n",
        "        if '/category/' in link or '/kategoria/' in link:\n",
        "            excluded['category'] += 1\n",
        "            continue\n",
        "        if '/tag/' in link:\n",
        "            excluded['tag'] += 1\n",
        "            continue\n",
        "        if '/author/' in link or '/autor/' in link:\n",
        "            excluded['author'] += 1\n",
        "            continue\n",
        "        if '/page/' in link or '/strona/' in link:\n",
        "            excluded['page'] += 1\n",
        "            continue\n",
        "\n",
        "        # Główna strona\n",
        "        if link in ['https://bukbuk.pl', 'https://www.bukbuk.pl', 'http://bukbuk.pl']:\n",
        "            excluded['other'] += 1\n",
        "            continue\n",
        "\n",
        "        # Wykluczamy linki które są podejrzanie krótkie lub tylko cyfry\n",
        "        path = link.replace('https://bukbuk.pl/', '').replace('https://www.bukbuk.pl/', '').replace('http://bukbuk.pl/', '')\n",
        "\n",
        "        # Jeśli path to tylko cyfry i myślniki\n",
        "        if re.match(r'^\\d+(-\\d+)?$', path):\n",
        "            excluded['digits'] += 1\n",
        "            continue\n",
        "\n",
        "        # Jeśli path jest bardzo krótki (< 8 znaków) i nie ma sensownych słów\n",
        "        if len(path) < 8 and not re.search(r'[a-z]{3,}', path, re.IGNORECASE):\n",
        "            excluded['short'] += 1\n",
        "            continue\n",
        "\n",
        "        article_links.append(link)\n",
        "\n",
        "    # Usuń duplikaty\n",
        "    article_links = list(set(article_links))\n",
        "\n",
        "    print(f\"\\nStatystyki filtrowania:\")\n",
        "    print(f\"  Kategorie: {excluded['category']}\")\n",
        "    print(f\"  Tagi: {excluded['tag']}\")\n",
        "    print(f\"  Autorzy: {excluded['author']}\")\n",
        "    print(f\"  Paginacja: {excluded['page']}\")\n",
        "    print(f\"  Same cyfry: {excluded['digits']}\")\n",
        "    print(f\"  Za krótkie: {excluded['short']}\")\n",
        "    print(f\"  Media/pliki: {excluded['media']}\")\n",
        "    print(f\"  Inne: {excluded['other']}\")\n",
        "    print(f\"  ✅ ZAAKCEPTOWANO: {len(article_links)}\")\n",
        "\n",
        "    return article_links\n",
        "\n",
        "\n",
        "#%% main execution\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_url = \"https://bukbuk.pl/\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Pobieranie linków z bukbuk.pl\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Pobierz wszystkie linki\n",
        "    all_links, errors = get_all_article_links(base_url)\n",
        "\n",
        "    print(f\"\\nZnaleziono {len(all_links)} linków\")\n",
        "\n",
        "    # Filtruj\n",
        "    article_links = filter_article_links(all_links)\n",
        "\n",
        "    if not article_links:\n",
        "        print(\"\\n⚠️  Nie znaleziono artykułów!\")\n",
        "        exit(1)\n",
        "\n",
        "    # Pokaż przykłady\n",
        "    print(\"\\nPrzykładowe linki (pierwsze 10):\")\n",
        "    for i, link in enumerate(sorted(article_links)[:10], 1):\n",
        "        print(f\"  {i}. {link}\")\n",
        "\n",
        "    if len(article_links) > 10:\n",
        "        print(f\"  ... i {len(article_links) - 10} więcej\")\n",
        "\n",
        "    # Sortuj\n",
        "    article_links.sort()\n",
        "\n",
        "    # Zapisz\n",
        "    with open('bukbuk_linki.txt', 'w', encoding='utf-8') as f:\n",
        "        for link in article_links:\n",
        "            f.write(link + '\\n')\n",
        "\n",
        "    output_data = {\n",
        "        'source': base_url,\n",
        "        'total_links': len(article_links),\n",
        "        'links': article_links,\n",
        "        'errors': errors\n",
        "    }\n",
        "\n",
        "    with open('bukbuk_linki.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Raport\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RAPORT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Znaleziono artykułów: {len(article_links)}\")\n",
        "    print(f\"Błędów: {len(errors)}\")\n",
        "    print(f\"\\nZapisano do:\")\n",
        "    print(f\"  - bukbuk_linki.txt\")\n",
        "    print(f\"  - bukbuk_linki.json\")\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xD5saofYZ0_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% import\n",
        "from __future__ import unicode_literals\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from time import mktime\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import json\n",
        "import xlsxwriter\n",
        "\n",
        "\n",
        "#%% functions\n",
        "\n",
        "def date_change_format(date_string):\n",
        "    \"\"\"\n",
        "    Konwertuje datę z różnych formatów na \"YYYY-MM-DD\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "        date_string = ' '.join(date_string.strip().split())\n",
        "\n",
        "        if re.match(r'\\d{4}-\\d{2}-\\d{2}', date_string):\n",
        "            return date_string[:10]\n",
        "\n",
        "        if 'T' in date_string:\n",
        "            return date_string.split('T')[0]\n",
        "\n",
        "        # Format DD.MM.YYYY (bukbuk.pl)\n",
        "        if re.match(r'\\d{2}\\.\\d{2}\\.\\d{4}', date_string):\n",
        "            result = time.strptime(date_string, \"%d.%m.%Y\")\n",
        "            changed_date = datetime.fromtimestamp(mktime(result))\n",
        "            return format(changed_date.date())\n",
        "\n",
        "        lookup_table = {\n",
        "            \"stycznia\": \"01\", \"lutego\": \"02\", \"marca\": \"03\", \"kwietnia\": \"04\",\n",
        "            \"maja\": \"05\", \"czerwca\": \"06\", \"lipca\": \"07\", \"sierpnia\": \"08\",\n",
        "            \"września\": \"09\", \"października\": \"10\", \"listopada\": \"11\", \"grudnia\": \"12\",\n",
        "            \"styczeń\": \"01\", \"luty\": \"02\", \"marzec\": \"03\", \"kwiecień\": \"04\",\n",
        "            \"maj\": \"05\", \"czerwiec\": \"06\", \"lipiec\": \"07\", \"sierpień\": \"08\",\n",
        "            \"wrzesień\": \"09\", \"październik\": \"10\", \"listopad\": \"11\", \"grudzień\": \"12\"\n",
        "        }\n",
        "\n",
        "        for k, v in lookup_table.items():\n",
        "            date_string = date_string.replace(k, v)\n",
        "\n",
        "        if re.match(r'\\d{1,2}\\.\\d{1,2}\\.\\d{4}', date_string):\n",
        "            result = time.strptime(date_string, \"%d.%m.%Y\")\n",
        "        else:\n",
        "            result = time.strptime(date_string, \"%d %m %Y\")\n",
        "\n",
        "        changed_date = datetime.fromtimestamp(mktime(result))\n",
        "        return format(changed_date.date())\n",
        "    except Exception as e:\n",
        "        return \"no date\"\n",
        "\n",
        "\n",
        "def dictionary_of_article(article_link):\n",
        "    \"\"\"\n",
        "    Pobiera szczegóły artykułu z bukbuk.pl\n",
        "    \"\"\"\n",
        "    try:\n",
        "        r = requests.get(article_link, timeout=15)\n",
        "        r.encoding = 'utf-8'\n",
        "\n",
        "        # Sprawdź czy strona istnieje\n",
        "        if r.status_code != 200:\n",
        "            print(f\"⚠️  HTTP {r.status_code} dla {article_link}\")\n",
        "            errors.append(article_link)\n",
        "            return\n",
        "\n",
        "        html_text = r.text\n",
        "\n",
        "        # Sprawdź czy to nie jest strona błędu\n",
        "        if 'error 404' in html_text.lower() or 'page not found' in html_text.lower():\n",
        "            print(f\"⚠️  404 dla {article_link}\")\n",
        "            errors.append(article_link)\n",
        "            return\n",
        "\n",
        "        soup = BeautifulSoup(html_text, 'lxml')\n",
        "\n",
        "        # Data publikacji\n",
        "        try:\n",
        "            # Opcja 1: post__date (bukbuk.pl)\n",
        "            date_element = soup.find('span', class_='post__date')\n",
        "\n",
        "            if not date_element:\n",
        "                date_element = soup.find('time')\n",
        "            if not date_element:\n",
        "                date_element = soup.find('span', class_=lambda x: x and 'date' in str(x).lower())\n",
        "            if not date_element:\n",
        "                date_element = soup.find(class_='entry-date')\n",
        "            if not date_element:\n",
        "                meta_date = soup.find('meta', property='article:published_time')\n",
        "                if meta_date:\n",
        "                    date_element = type('obj', (object,), {\n",
        "                        'get_text': lambda: meta_date.get('content', ''),\n",
        "                        'get': lambda x: meta_date.get('content', '')\n",
        "                    })()\n",
        "\n",
        "            if date_element:\n",
        "                date_text = date_element.get('datetime') or date_element.get('content') or date_element.get_text(strip=True)\n",
        "                date_of_publication = date_change_format(date_text)\n",
        "            else:\n",
        "                date_of_publication = \"no date\"\n",
        "        except Exception as e:\n",
        "            date_of_publication = \"no date\"\n",
        "\n",
        "        # Tytuł\n",
        "        try:\n",
        "            title_element = soup.find('h1', class_=lambda x: x and 'entry-title' in str(x).lower())\n",
        "            if not title_element:\n",
        "                title_element = soup.find('h1')\n",
        "            if not title_element:\n",
        "                title_element = soup.find('title')\n",
        "\n",
        "            if title_element:\n",
        "                title = title_element.get_text(strip=True)\n",
        "                # Usuń \"| Bukbuk.pl\" itp.\n",
        "                title = re.sub(r'\\s*[-|]\\s*[Bb]ukbuk\\.pl.*$', '', title)\n",
        "                title = title.strip()\n",
        "\n",
        "            if not title:\n",
        "                title = \"no title\"\n",
        "        except:\n",
        "            title = \"no title\"\n",
        "\n",
        "        # Autor\n",
        "        try:\n",
        "            # Opcja 1: post__author (bukbuk.pl)\n",
        "            author_element = soup.find('span', class_='post__author')\n",
        "\n",
        "            if not author_element:\n",
        "                author_element = soup.find('a', rel='author')\n",
        "            if not author_element:\n",
        "                author_element = soup.find('span', class_=lambda x: x and 'author' in str(x).lower())\n",
        "            if not author_element:\n",
        "                author_element = soup.find(class_='author-name')\n",
        "\n",
        "            if author_element:\n",
        "                author = author_element.get_text(strip=True)\n",
        "                author = re.sub(r'^(Autor|By|Opublikował):\\s*', '', author, flags=re.IGNORECASE)\n",
        "            else:\n",
        "                author = \"no author\"\n",
        "        except:\n",
        "            author = \"no author\"\n",
        "\n",
        "        # Treść artykułu\n",
        "        try:\n",
        "            # Bukbuk ma specyficzną strukturę: post__lead + post__content\n",
        "            text_parts = []\n",
        "\n",
        "            # Opcja 1: post__lead (główna treść w bukbuk.pl)\n",
        "            lead = soup.find('div', class_='post__lead')\n",
        "            if lead:\n",
        "                text_parts.append(lead.get_text(strip=True))\n",
        "\n",
        "            # Opcja 2: post__content (dodatkowa treść)\n",
        "            content = soup.find('div', class_='post__content')\n",
        "            if content:\n",
        "                text_parts.append(content.get_text(strip=True))\n",
        "\n",
        "            # Opcja 3: Standardowe entry-content\n",
        "            if not text_parts:\n",
        "                article_body = soup.find('div', class_=lambda x: x and 'entry-content' in str(x).lower())\n",
        "                if article_body:\n",
        "                    text_parts.append(article_body.get_text(strip=True))\n",
        "\n",
        "            # Opcja 4: post-content\n",
        "            if not text_parts:\n",
        "                article_body = soup.find('div', class_=lambda x: x and 'post-content' in str(x).lower())\n",
        "                if article_body:\n",
        "                    text_parts.append(article_body.get_text(strip=True))\n",
        "\n",
        "            # Opcja 5: article\n",
        "            if not text_parts:\n",
        "                article_elem = soup.find('article')\n",
        "                if article_elem:\n",
        "                    content_div = article_elem.find('div', class_=lambda x: x and 'content' in str(x).lower())\n",
        "                    if content_div:\n",
        "                        text_parts.append(content_div.get_text(strip=True))\n",
        "\n",
        "            if text_parts:\n",
        "                text = ' '.join(text_parts)\n",
        "                text = text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
        "                text = re.sub(r'\\s+', ' ', text)\n",
        "            else:\n",
        "                text = \"no text\"\n",
        "        except Exception as e:\n",
        "            text = \"no text\"\n",
        "\n",
        "        # Kategoria\n",
        "        try:\n",
        "            category_links = soup.find_all('a', rel='category tag')\n",
        "            if not category_links:\n",
        "                category_links = soup.find_all('a', rel='category')\n",
        "\n",
        "            if category_links:\n",
        "                categories = [cat.get_text(strip=True) for cat in category_links]\n",
        "                category = ' | '.join(categories)\n",
        "            else:\n",
        "                category = \"no category\"\n",
        "        except:\n",
        "            category = \"no category\"\n",
        "\n",
        "        # Tagi\n",
        "        try:\n",
        "            tag_links = soup.find_all('a', rel='tag')\n",
        "            if tag_links:\n",
        "                tags = [tag.get_text(strip=True) for tag in tag_links]\n",
        "                tags_str = ' | '.join(tags)\n",
        "            else:\n",
        "                tags_str = None\n",
        "        except:\n",
        "            tags_str = None\n",
        "\n",
        "        # Linki zewnętrzne\n",
        "        try:\n",
        "            links = []\n",
        "            # Zbierz z post__lead\n",
        "            lead = soup.find('div', class_='post__lead')\n",
        "            if lead:\n",
        "                links.extend([a['href'] for a in lead.find_all('a', href=True)])\n",
        "\n",
        "            # Zbierz z post__content\n",
        "            content = soup.find('div', class_='post__content')\n",
        "            if content:\n",
        "                links.extend([a['href'] for a in content.find_all('a', href=True)])\n",
        "\n",
        "            external_links = [link for link in links if not re.search(r'bukbuk\\.pl', link)]\n",
        "            external_links = ' | '.join(external_links) if external_links else None\n",
        "        except (AttributeError, KeyError, IndexError):\n",
        "            external_links = None\n",
        "\n",
        "        # Zdjęcia\n",
        "        try:\n",
        "            images = []\n",
        "\n",
        "            # Thumbnail/Featured\n",
        "            thumbnail_div = soup.find('div', class_=lambda x: x and 'post-thumbnail' in str(x).lower())\n",
        "            if thumbnail_div:\n",
        "                thumb_img = thumbnail_div.find('img')\n",
        "                if thumb_img:\n",
        "                    img_url = thumb_img.get('src') or thumb_img.get('data-src')\n",
        "                    if img_url and img_url not in images:\n",
        "                        images.append(img_url)\n",
        "\n",
        "            # Z post__lead\n",
        "            lead = soup.find('div', class_='post__lead')\n",
        "            if lead:\n",
        "                for img in lead.find_all('img'):\n",
        "                    img_url = img.get('src') or img.get('data-src')\n",
        "                    if img_url and img_url not in images:\n",
        "                        images.append(img_url)\n",
        "\n",
        "            # Z post__content\n",
        "            content = soup.find('div', class_='post__content')\n",
        "            if content:\n",
        "                for img in content.find_all('img'):\n",
        "                    img_url = img.get('src') or img.get('data-src')\n",
        "                    if img_url and img_url not in images:\n",
        "                        images.append(img_url)\n",
        "\n",
        "            has_images = len(images) > 0\n",
        "            photos_links = ' | '.join(images) if images else None\n",
        "        except (AttributeError, KeyError, IndexError):\n",
        "            has_images = False\n",
        "            photos_links = None\n",
        "\n",
        "        # Filmy\n",
        "        try:\n",
        "            iframes = []\n",
        "            # Z post__lead\n",
        "            lead = soup.find('div', class_='post__lead')\n",
        "            if lead:\n",
        "                iframes.extend([iframe['src'] for iframe in lead.find_all('iframe', src=True)])\n",
        "\n",
        "            # Z post__content\n",
        "            content = soup.find('div', class_='post__content')\n",
        "            if content:\n",
        "                iframes.extend([iframe['src'] for iframe in content.find_all('iframe', src=True)])\n",
        "\n",
        "            has_videos = len(iframes) > 0\n",
        "        except:\n",
        "            has_videos = False\n",
        "\n",
        "        dictionary_of_article = {\n",
        "            \"Link\": article_link,\n",
        "            \"Data publikacji\": date_of_publication,\n",
        "            \"Tytuł artykułu\": title.replace('\\xa0', ' '),\n",
        "            \"Tekst artykułu\": text,\n",
        "            \"Autor\": author,\n",
        "            \"Kategoria\": category,\n",
        "            \"Tagi\": tags_str,\n",
        "            \"Linki zewnętrzne\": external_links,\n",
        "            \"Zdjęcia/Grafika\": has_images,\n",
        "            \"Filmy\": has_videos,\n",
        "            \"Linki do zdjęć\": photos_links\n",
        "        }\n",
        "\n",
        "        all_results.append(dictionary_of_article)\n",
        "\n",
        "    except AttributeError as e:\n",
        "        errors.append(article_link)\n",
        "        print(f\"Błąd dla {article_link}: {e}\")\n",
        "    except Exception as e:\n",
        "        errors.append(article_link)\n",
        "        print(f\"Nieoczekiwany błąd dla {article_link}: {e}\")\n",
        "\n",
        "\n",
        "#%% main execution\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Wczytaj linki\n",
        "    try:\n",
        "        with open('bukbuk_linki.txt', 'r', encoding='utf-8') as f:\n",
        "            article_links = [line.strip() for line in f if line.strip()]\n",
        "        print(f\"Wczytano {len(article_links)} linków z pliku\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Nie znaleziono pliku bukbuk_linki.txt\")\n",
        "        print(\"Użyj najpierw get_bukbuk_links.py!\")\n",
        "        article_links = []\n",
        "\n",
        "    if not article_links:\n",
        "        print(\"Brak linków do przetworzenia!\")\n",
        "        exit(1)\n",
        "\n",
        "    all_results = []\n",
        "    errors = []\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Rozpoczynam scraping artykułów z bukbuk.pl\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Scraping z wieloma wątkami\n",
        "    max_workers = 10\n",
        "    print(f\"Używam {max_workers} równoległych wątków\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        list(tqdm(executor.map(dictionary_of_article, article_links), total=len(article_links)))\n",
        "\n",
        "    # Zapisywanie\n",
        "    timestamp = datetime.today().date()\n",
        "\n",
        "    # JSON\n",
        "    with open(f'bukbuk_{timestamp}.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)\n",
        "\n",
        "    # Excel\n",
        "    df = pd.DataFrame(all_results)\n",
        "    with pd.ExcelWriter(f\"bukbuk_{timestamp}.xlsx\",\n",
        "                       engine='xlsxwriter',\n",
        "                       engine_kwargs={'options': {'strings_to_urls': False}}) as writer:\n",
        "        df.to_excel(writer, 'Posts', index=False)\n",
        "\n",
        "    # Raport\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Scraping zakończony!\")\n",
        "    print(f\"Przetworzono artykułów: {len(all_results)}\")\n",
        "    print(f\"Błędów: {len(errors)}\")\n",
        "    if errors:\n",
        "        print(f\"\\nLinki z błędami (pierwsze 10):\")\n",
        "        for error_link in errors[:10]:\n",
        "            print(f\"  - {error_link}\")\n",
        "        if len(errors) > 10:\n",
        "            print(f\"  ... i {len(errors) - 10} więcej\")\n",
        "    print(f\"\\nPliki wyjściowe:\")\n",
        "    print(f\"  - bukbuk_{timestamp}.json\")\n",
        "    print(f\"  - bukbuk_{timestamp}.xlsx\")\n",
        "    print(f\"{'='*60}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgsbrjT1mnLq",
        "outputId": "5d2f2b91-aa26-4b1c-946b-0dc2b382dc80"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wczytano 207 linków z pliku\n",
            "\n",
            "============================================================\n",
            "Rozpoczynam scraping artykułów z bukbuk.pl\n",
            "============================================================\n",
            "\n",
            "Używam 10 równoległych wątków\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 207/207 [00:26<00:00,  7.76it/s]\n",
            "/tmp/ipython-input-2729046711.py:356: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  df.to_excel(writer, 'Posts', index=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Scraping zakończony!\n",
            "Przetworzono artykułów: 207\n",
            "Błędów: 0\n",
            "\n",
            "Pliki wyjściowe:\n",
            "  - bukbuk_2026-01-12.json\n",
            "  - bukbuk_2026-01-12.xlsx\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}