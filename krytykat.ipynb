{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter pydrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvBrlmI4uyYC",
        "outputId": "16cd9aa7-56a8-4110-bab8-63ac61cefbe0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pydrive\n",
            "  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.12/dist-packages (from pydrive) (2.187.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.12/dist-packages (from pydrive) (6.0.3)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (0.31.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (2.43.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (0.2.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (2.28.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (4.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (4.9.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (1.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.72.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.2->pydrive) (6.2.4)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=1.2->pydrive) (3.2.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2025.11.12)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pydrive\n",
            "  Building wheel for pydrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pydrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27433 sha256=428ed61acdc0079692a666efffca1a436cd5d18bda97115b7f83755653899c7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/10/da/a5b513f5b3916fc391c20ee7b4633e5cf3396d570cdd74970f\n",
            "Successfully built pydrive\n",
            "Installing collected packages: xlsxwriter, pydrive\n",
            "Successfully installed pydrive-1.3.1 xlsxwriter-3.2.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYYbalsWuwAs"
      },
      "outputs": [],
      "source": [
        "#%% import\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "#%% functions\n",
        "\n",
        "def get_sitemap_links(sitemap_url):\n",
        "    \"\"\"\n",
        "    Pobiera linki z sitemap.xml\n",
        "    \"\"\"\n",
        "    try:\n",
        "        r = requests.get(sitemap_url)\n",
        "        r.encoding = 'utf-8'\n",
        "        soup = BeautifulSoup(r.text, 'xml')  # Użyj parsera XML!\n",
        "\n",
        "        # W sitemap.xml linki są w tagach <loc>\n",
        "        links = [loc.text.strip() for loc in soup.find_all('loc')]\n",
        "\n",
        "        return links\n",
        "    except Exception as e:\n",
        "        print(f\"Błąd pobierania sitemap: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def filter_article_links(all_links):\n",
        "    \"\"\"\n",
        "    Filtruje linki - zostawia tylko artykuły\n",
        "    WordPress.com struktura artykułów: /YYYY/MM/DD/slug/ lub /YYYY/MM/slug/\n",
        "    \"\"\"\n",
        "    article_links = []\n",
        "    excluded_count = {'sitemap': 0, 'category': 0, 'tag': 0, 'page': 0, 'author': 0, 'files': 0, 'other': 0}\n",
        "\n",
        "    for link in all_links:\n",
        "        # 0. Wykluczamy pliki (obrazy, PDF, etc.)\n",
        "        file_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.pdf', '.doc', '.docx', '.zip', '.mp4', '.mp3']\n",
        "        if any(link.lower().endswith(ext) for ext in file_extensions):\n",
        "            excluded_count['files'] += 1\n",
        "            continue\n",
        "\n",
        "        # Wykluczamy wp-content (uploads)\n",
        "        if '/wp-content/' in link:\n",
        "            excluded_count['files'] += 1\n",
        "            continue\n",
        "\n",
        "        # 1. Wykluczamy sitemapy\n",
        "        if 'sitemap' in link.lower():\n",
        "            excluded_count['sitemap'] += 1\n",
        "            continue\n",
        "\n",
        "        # 2. Wykluczamy kategorie i tagi\n",
        "        if '/category/' in link or '/tag/' in link:\n",
        "            excluded_count['category'] += 1\n",
        "            continue\n",
        "\n",
        "        # 3. Wykluczamy autorów\n",
        "        if '/author/' in link:\n",
        "            excluded_count['author'] += 1\n",
        "            continue\n",
        "\n",
        "        # 4. Wykluczamy strony paginacji\n",
        "        if '/page/' in link:\n",
        "            excluded_count['page'] += 1\n",
        "            continue\n",
        "\n",
        "        # 5. Wykluczamy archiwum dat (tylko /YYYY/MM/ bez dalszej ścieżki)\n",
        "        # Wzorzec: kończy się na /YYYY/MM/ lub /YYYY/\n",
        "        if re.match(r'.*\\/\\d{4}\\/(0\\d|1[0-2])\\/?$', link):\n",
        "            excluded_count['other'] += 1\n",
        "            continue\n",
        "        if re.match(r'.*\\/\\d{4}\\/?$', link):\n",
        "            excluded_count['other'] += 1\n",
        "            continue\n",
        "\n",
        "        # 6. AKCEPTUJEMY: artykuły z datą w ścieżce\n",
        "        # WordPress zwykle: /YYYY/MM/DD/slug/ lub /YYYY/MM/slug/\n",
        "        # Musi mieć coś PO dacie (slug artykułu)\n",
        "        if re.search(r'\\/\\d{4}\\/\\d{2}\\/.+\\/', link):\n",
        "            article_links.append(link)\n",
        "            continue\n",
        "\n",
        "        # 7. Wykluczamy główną stronę, about, contact, etc.\n",
        "        # Jeśli URL jest bardzo krótki (tylko domena) lub statyczne strony\n",
        "        path = link.replace('https://krytykat.wordpress.com', '').strip('/')\n",
        "        if not path or path in ['about', 'contact', 'o-blogu', 'archiwum']:\n",
        "            excluded_count['other'] += 1\n",
        "            continue\n",
        "\n",
        "        # 8. Jeśli nie pasuje do żadnej kategorii ale wygląda jak artykuł\n",
        "        # (ma tytuł w URL, nie jest krótki)\n",
        "        if len(path.split('/')) >= 2:  # Przynajmniej 2 segmenty\n",
        "            article_links.append(link)\n",
        "\n",
        "    # Raport filtrowania\n",
        "    print(f\"\\nStatystyki filtrowania:\")\n",
        "    print(f\"  Pliki (obrazy, PDF, etc.): {excluded_count['files']}\")\n",
        "    print(f\"  Sitemapy: {excluded_count['sitemap']}\")\n",
        "    print(f\"  Kategorie/tagi: {excluded_count['category']}\")\n",
        "    print(f\"  Autorzy: {excluded_count['author']}\")\n",
        "    print(f\"  Paginacja: {excluded_count['page']}\")\n",
        "    print(f\"  Inne (archiwum, strony): {excluded_count['other']}\")\n",
        "    print(f\"  ✅ ZAAKCEPTOWANO: {len(article_links)}\")\n",
        "\n",
        "    return article_links\n",
        "\n",
        "\n",
        "def get_all_article_links(sitemap_url):\n",
        "    \"\"\"\n",
        "    Główna funkcja - pobiera wszystkie linki z sitemap\n",
        "    \"\"\"\n",
        "    print(\"Krok 1: Pobieranie sitemap...\")\n",
        "    all_links = get_sitemap_links(sitemap_url)\n",
        "\n",
        "    print(f\"Znaleziono {len(all_links)} linków w sitemap\")\n",
        "\n",
        "    print(\"\\nKrok 2: Filtrowanie linków do artykułów...\")\n",
        "    article_links = filter_article_links(all_links)\n",
        "\n",
        "    print(f\"Po filtrowaniu: {len(article_links)} artykułów\")\n",
        "\n",
        "    # Pokaż przykłady\n",
        "    if article_links:\n",
        "        print(\"\\nPrzykładowe linki (pierwsze 5):\")\n",
        "        for i, link in enumerate(article_links[:5], 1):\n",
        "            print(f\"  {i}. {link}\")\n",
        "\n",
        "    return article_links, []\n",
        "\n",
        "\n",
        "#%% main execution\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sitemap_url = \"https://krytykat.wordpress.com/sitemap.xml\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Pobieranie linków z sitemap krytykat.wordpress.com\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Pobierz wszystkie linki\n",
        "    article_links, errors = get_all_article_links(sitemap_url)\n",
        "\n",
        "    if not article_links:\n",
        "        print(\"\\n⚠️  Nie znaleziono artykułów!\")\n",
        "        print(\"Sprawdź ręcznie sitemap:\")\n",
        "        print(f\"  {sitemap_url}\")\n",
        "        exit(1)\n",
        "\n",
        "    # Sortuj alfabetycznie\n",
        "    article_links.sort()\n",
        "\n",
        "    # Zapisz do pliku tekstowego\n",
        "    with open('krytykat_linki.txt', 'w', encoding='utf-8') as f:\n",
        "        for link in article_links:\n",
        "            f.write(link + '\\n')\n",
        "\n",
        "    # Zapisz do JSON (z metadanymi)\n",
        "    output_data = {\n",
        "        'source': sitemap_url,\n",
        "        'total_links': len(article_links),\n",
        "        'links': article_links,\n",
        "        'errors': errors\n",
        "    }\n",
        "\n",
        "    with open('krytykat_linki.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Raport\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RAPORT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Znaleziono artykułów: {len(article_links)}\")\n",
        "    print(f\"\\nZapisano do:\")\n",
        "    print(f\"  - krytykat_linki.txt\")\n",
        "    print(f\"  - krytykat_linki.json\")\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%% import\n",
        "from __future__ import unicode_literals\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from time import mktime\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import json\n",
        "import xlsxwriter\n",
        "\n",
        "\n",
        "#%% functions\n",
        "\n",
        "def date_change_format(date_string):\n",
        "    \"\"\"\n",
        "    Konwertuje datę z różnych formatów na \"YYYY-MM-DD\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "        date_string = ' '.join(date_string.strip().split())\n",
        "\n",
        "        # Jeśli już jest w formacie YYYY-MM-DD\n",
        "        if re.match(r'\\d{4}-\\d{2}-\\d{2}', date_string):\n",
        "            return date_string[:10]\n",
        "\n",
        "        # Jeśli jest datetime z czasem\n",
        "        if 'T' in date_string:\n",
        "            return date_string.split('T')[0]\n",
        "\n",
        "        # Słownik z obiema formami miesięcy\n",
        "        lookup_table = {\n",
        "            \"stycznia\": \"01\", \"lutego\": \"02\", \"marca\": \"03\", \"kwietnia\": \"04\",\n",
        "            \"maja\": \"05\", \"czerwca\": \"06\", \"lipca\": \"07\", \"sierpnia\": \"08\",\n",
        "            \"września\": \"09\", \"października\": \"10\", \"listopada\": \"11\", \"grudnia\": \"12\",\n",
        "            \"styczeń\": \"01\", \"luty\": \"02\", \"marzec\": \"03\", \"kwiecień\": \"04\",\n",
        "            \"maj\": \"05\", \"czerwiec\": \"06\", \"lipiec\": \"07\", \"sierpień\": \"08\",\n",
        "            \"wrzesień\": \"09\", \"październik\": \"10\", \"listopad\": \"11\", \"grudzień\": \"12\"\n",
        "        }\n",
        "\n",
        "        for k, v in lookup_table.items():\n",
        "            date_string = date_string.replace(k, v)\n",
        "\n",
        "        if re.match(r'\\d{1,2}\\.\\d{1,2}\\.\\d{4}', date_string):\n",
        "            result = time.strptime(date_string, \"%d.%m.%Y\")\n",
        "        else:\n",
        "            result = time.strptime(date_string, \"%d %m %Y\")\n",
        "\n",
        "        changed_date = datetime.fromtimestamp(mktime(result))\n",
        "        return format(changed_date.date())\n",
        "    except Exception as e:\n",
        "        print(f\"Błąd konwersji daty '{date_string}': {e}\")\n",
        "        return \"no date\"\n",
        "\n",
        "\n",
        "def dictionary_of_article(article_link):\n",
        "    \"\"\"\n",
        "    Pobiera szczegóły artykułu z krytykat.wordpress.com\n",
        "    \"\"\"\n",
        "    try:\n",
        "        r = requests.get(article_link)\n",
        "        r.encoding = 'utf-8'\n",
        "        html_text = r.text\n",
        "\n",
        "        while '429 Too Many Requests' in html_text:\n",
        "            time.sleep(5)\n",
        "            r = requests.get(article_link)\n",
        "            r.encoding = 'utf-8'\n",
        "            html_text = r.text\n",
        "\n",
        "        soup = BeautifulSoup(html_text, 'lxml')\n",
        "\n",
        "        # Data publikacji\n",
        "        try:\n",
        "            # WordPress.com może mieć różne struktury\n",
        "            date_element = soup.find('time')  # Bez klasy - szukamy dowolnego <time>\n",
        "            if date_element:\n",
        "                # Próbuj datetime attribute (najlepsze)\n",
        "                date_text = date_element.get('datetime')\n",
        "                if not date_text:\n",
        "                    # Jeśli nie ma datetime, użyj tekstu\n",
        "                    date_text = date_element.get_text(strip=True)\n",
        "                date_of_publication = date_change_format(date_text)\n",
        "            else:\n",
        "                # Alternatywnie szukaj w spanach/divach z \"date\"\n",
        "                date_element = soup.find(['span', 'div'], class_=lambda x: x and 'date' in str(x).lower())\n",
        "                if date_element:\n",
        "                    date_text = date_element.get_text(strip=True)\n",
        "                    date_of_publication = date_change_format(date_text)\n",
        "                else:\n",
        "                    date_of_publication = \"no date\"\n",
        "        except Exception as e:\n",
        "            print(f\"Błąd parsowania daty dla {article_link}: {e}\")\n",
        "            date_of_publication = \"no date\"\n",
        "\n",
        "        # Tytuł\n",
        "        try:\n",
        "            title_element = soup.find('h1', class_='entry-title')\n",
        "            if not title_element:\n",
        "                title_element = soup.find('h1')\n",
        "            title = title_element.get_text(strip=True) if title_element else \"no title\"\n",
        "        except:\n",
        "            title = \"no title\"\n",
        "\n",
        "        # Autor\n",
        "        try:\n",
        "            author_element = soup.find('a', class_='url fn n')\n",
        "            if not author_element:\n",
        "                author_element = soup.find('span', class_='author')\n",
        "            if not author_element:\n",
        "                author_element = soup.find('a', rel='author')\n",
        "\n",
        "            author = author_element.get_text(strip=True) if author_element else \"no author\"\n",
        "        except:\n",
        "            author = \"no author\"\n",
        "\n",
        "        # Treść artykułu\n",
        "        try:\n",
        "            article_body = soup.find('div', class_='entry-content')\n",
        "            if article_body:\n",
        "                text = article_body.get_text(strip=True).replace('\\n', ' ').replace('\\xa0', ' ')\n",
        "            else:\n",
        "                text = \"no text\"\n",
        "        except:\n",
        "            text = \"no text\"\n",
        "\n",
        "        # Kategoria\n",
        "        try:\n",
        "            category_links = soup.find_all('a', rel='category tag')\n",
        "            if not category_links:\n",
        "                category_links = soup.find_all('a', rel='category')\n",
        "\n",
        "            if category_links:\n",
        "                categories = [cat.get_text(strip=True) for cat in category_links]\n",
        "                category = ' | '.join(categories)\n",
        "            else:\n",
        "                category = \"no category\"\n",
        "        except:\n",
        "            category = \"no category\"\n",
        "\n",
        "        # Tagi\n",
        "        try:\n",
        "            tag_links = soup.find_all('a', rel='tag')\n",
        "            if tag_links:\n",
        "                tags = [tag.get_text(strip=True) for tag in tag_links]\n",
        "                tags_str = ' | '.join(tags)\n",
        "            else:\n",
        "                tags_str = None\n",
        "        except:\n",
        "            tags_str = None\n",
        "\n",
        "        # Linki zewnętrzne\n",
        "        try:\n",
        "            if article_body:\n",
        "                links = [a['href'] for a in article_body.find_all('a', href=True)]\n",
        "                external_links = [link for link in links if not re.search(r'krytykat\\.wordpress\\.com|wordpress\\.com', link)]\n",
        "                external_links = ' | '.join(external_links) if external_links else None\n",
        "            else:\n",
        "                external_links = None\n",
        "        except (AttributeError, KeyError, IndexError):\n",
        "            external_links = None\n",
        "\n",
        "        # Zdjęcia\n",
        "        try:\n",
        "            images = []\n",
        "\n",
        "            # 1. Thumbnail / post-thumbnail\n",
        "            thumbnail_div = soup.find('div', class_=lambda x: x and 'post-thumbnail' in str(x).lower())\n",
        "            if thumbnail_div:\n",
        "                thumb_img = thumbnail_div.find('img', src=True)\n",
        "                if thumb_img:\n",
        "                    images.append(thumb_img['src'])\n",
        "\n",
        "            # 2. Featured image\n",
        "            if not images:\n",
        "                featured_img = soup.find('img', class_=lambda x: x and 'wp-post-image' in str(x).lower())\n",
        "                if featured_img and featured_img.get('src'):\n",
        "                    images.append(featured_img['src'])\n",
        "\n",
        "            # 3. Zdjęcia w treści\n",
        "            if article_body:\n",
        "                content_images = [img['src'] for img in article_body.find_all('img', src=True) if img.get('src')]\n",
        "                for img_src in content_images:\n",
        "                    if img_src not in images:\n",
        "                        images.append(img_src)\n",
        "\n",
        "            # 4. Inne miejsca\n",
        "            for container_class in ['entry-header', 'post-header']:\n",
        "                header = soup.find('div', class_=container_class)\n",
        "                if header:\n",
        "                    header_images = [img['src'] for img in header.find_all('img', src=True) if img.get('src')]\n",
        "                    for img_src in header_images:\n",
        "                        if img_src not in images:\n",
        "                            images.append(img_src)\n",
        "\n",
        "            has_images = len(images) > 0\n",
        "            photos_links = ' | '.join(images) if images else None\n",
        "        except (AttributeError, KeyError, IndexError) as e:\n",
        "            has_images = False\n",
        "            photos_links = None\n",
        "\n",
        "        # Filmy (iframe)\n",
        "        try:\n",
        "            if article_body:\n",
        "                iframes = [iframe['src'] for iframe in article_body.find_all('iframe', src=True)]\n",
        "                has_videos = len(iframes) > 0\n",
        "            else:\n",
        "                has_videos = False\n",
        "        except:\n",
        "            has_videos = False\n",
        "\n",
        "        dictionary_of_article = {\n",
        "            \"Link\": article_link,\n",
        "            \"Data publikacji\": date_of_publication,\n",
        "            \"Tytuł artykułu\": title.replace('\\xa0', ' '),\n",
        "            \"Tekst artykułu\": text,\n",
        "            \"Autor\": author,\n",
        "            \"Kategoria\": category,\n",
        "            \"Tagi\": tags_str,\n",
        "            \"Linki zewnętrzne\": external_links,\n",
        "            \"Zdjęcia/Grafika\": has_images,\n",
        "            \"Filmy\": has_videos,\n",
        "            \"Linki do zdjęć\": photos_links\n",
        "        }\n",
        "\n",
        "        all_results.append(dictionary_of_article)\n",
        "\n",
        "    except AttributeError as e:\n",
        "        errors.append(article_link)\n",
        "        print(f\"Błąd dla {article_link}: {e}\")\n",
        "    except Exception as e:\n",
        "        errors.append(article_link)\n",
        "        print(f\"Nieoczekiwany błąd dla {article_link}: {e}\")\n",
        "\n",
        "\n",
        "#%% main execution\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Wczytaj linki z pliku\n",
        "    try:\n",
        "        with open('krytykat_linki.txt', 'r', encoding='utf-8') as f:\n",
        "            article_links = [line.strip() for line in f if line.strip()]\n",
        "        print(f\"Wczytano {len(article_links)} linków z pliku\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Nie znaleziono pliku krytykat_linki.txt\")\n",
        "        print(\"Użyj najpierw get_krytykat_links.py aby pobrać linki!\")\n",
        "        print(\"\\nLub podaj linki ręcznie:\")\n",
        "        article_links = []\n",
        "\n",
        "    if not article_links:\n",
        "        print(\"Brak linków do przetworzenia!\")\n",
        "        exit(1)\n",
        "\n",
        "    all_results = []\n",
        "    errors = []\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Rozpoczynam scraping artykułów z krytykat.wordpress.com\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Scraping z progress barem\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        list(tqdm(executor.map(dictionary_of_article, article_links), total=len(article_links)))\n",
        "\n",
        "    # Zapisywanie wyników\n",
        "    timestamp = datetime.today().date()\n",
        "\n",
        "    # JSON\n",
        "    with open(f'krytykat_{timestamp}.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)\n",
        "\n",
        "    # Excel\n",
        "    df = pd.DataFrame(all_results)\n",
        "    with pd.ExcelWriter(f\"krytykat_{timestamp}.xlsx\",\n",
        "                       engine='xlsxwriter',\n",
        "                       engine_kwargs={'options': {'strings_to_urls': False}}) as writer:\n",
        "        df.to_excel(writer, 'Posts', index=False)\n",
        "\n",
        "    # Raport\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Scraping zakończony!\")\n",
        "    print(f\"Przetworzono artykułów: {len(all_results)}\")\n",
        "    print(f\"Błędów: {len(errors)}\")\n",
        "    if errors:\n",
        "        print(f\"\\nLinki z błędami (pierwsze 10):\")\n",
        "        for error_link in errors[:10]:\n",
        "            print(f\"  - {error_link}\")\n",
        "        if len(errors) > 10:\n",
        "            print(f\"  ... i {len(errors) - 10} więcej\")\n",
        "    print(f\"\\nPliki wyjściowe:\")\n",
        "    print(f\"  - krytykat_{timestamp}.json\")\n",
        "    print(f\"  - krytykat_{timestamp}.xlsx\")\n",
        "    print(f\"{'='*60}\\n\")"
      ],
      "metadata": {
        "id": "GEgvZdTEws19"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}