{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter pydrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfmKQTx9pyoF",
        "outputId": "6afa79b1-63cd-4dc7-8047-8789bdfaac14"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pydrive\n",
            "  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.12/dist-packages (from pydrive) (2.187.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.12/dist-packages (from pydrive) (6.0.3)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (0.31.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (2.43.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (0.3.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (2.29.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.2->pydrive) (4.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (4.9.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=4.0.0->pydrive) (1.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.72.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.27.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.2->pydrive) (6.2.4)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=1.2->pydrive) (3.3.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2026.1.4)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pydrive\n",
            "  Building wheel for pydrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pydrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27433 sha256=9331887b11b38668cce0850c9024f0a07c9ffd636f7ee1be4dc67f9df36166bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/10/da/a5b513f5b3916fc391c20ee7b4633e5cf3396d570cdd74970f\n",
            "Successfully built pydrive\n",
            "Installing collected packages: xlsxwriter, pydrive\n",
            "Successfully installed pydrive-1.3.1 xlsxwriter-3.2.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGID3bdKpxb7"
      },
      "outputs": [],
      "source": [
        "#%% import\n",
        "from __future__ import unicode_literals\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from time import mktime\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import json\n",
        "import xlsxwriter\n",
        "\n",
        "\n",
        "#%% functions - data conversion\n",
        "\n",
        "def date_change_format(date_string):\n",
        "    \"\"\"\n",
        "    Konwertuje datę z różnych formatów na \"YYYY-MM-DD\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "        date_string = ' '.join(date_string.strip().split())\n",
        "\n",
        "        if re.match(r'\\d{4}-\\d{2}-\\d{2}', date_string):\n",
        "            return date_string[:10]\n",
        "\n",
        "        if 'T' in date_string:\n",
        "            return date_string.split('T')[0]\n",
        "\n",
        "        if re.match(r'\\d{2}\\.\\d{2}\\.\\d{4}', date_string):\n",
        "            result = time.strptime(date_string, \"%d.%m.%Y\")\n",
        "            changed_date = datetime.fromtimestamp(mktime(result))\n",
        "            return format(changed_date.date())\n",
        "\n",
        "        lookup_table = {\n",
        "            \"stycznia\": \"01\", \"lutego\": \"02\", \"marca\": \"03\", \"kwietnia\": \"04\",\n",
        "            \"maja\": \"05\", \"czerwca\": \"06\", \"lipca\": \"07\", \"sierpnia\": \"08\",\n",
        "            \"września\": \"09\", \"października\": \"10\", \"listopada\": \"11\", \"grudnia\": \"12\",\n",
        "            \"styczeń\": \"01\", \"luty\": \"02\", \"marzec\": \"03\", \"kwiecień\": \"04\",\n",
        "            \"maj\": \"05\", \"czerwiec\": \"06\", \"lipiec\": \"07\", \"sierpień\": \"08\",\n",
        "            \"wrzesień\": \"09\", \"październik\": \"10\", \"listopad\": \"11\", \"grudzień\": \"12\"\n",
        "        }\n",
        "\n",
        "        for k, v in lookup_table.items():\n",
        "            date_string = date_string.replace(k, v)\n",
        "\n",
        "        if re.match(r'\\d{1,2}\\.\\d{1,2}\\.\\d{4}', date_string):\n",
        "            result = time.strptime(date_string, \"%d.%m.%Y\")\n",
        "        else:\n",
        "            result = time.strptime(date_string, \"%d %m %Y\")\n",
        "\n",
        "        changed_date = datetime.fromtimestamp(mktime(result))\n",
        "        return format(changed_date.date())\n",
        "    except Exception as e:\n",
        "        return \"no date\"\n",
        "\n",
        "\n",
        "#%% functions - link extraction\n",
        "\n",
        "def get_all_article_links():\n",
        "    \"\"\"\n",
        "    Pobiera wszystkie linki do artykułów z portalkryminalny.pl\n",
        "    Struktura: <div class=\"news\" onclick=\"location.href='aktualnosci/kategoria/tytul'\">\n",
        "    \"\"\"\n",
        "    base_url = \"https://portalkryminalny.pl\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"KROK 1: Pobieranie linków do artykułów\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    all_links = set()\n",
        "\n",
        "    # Kategorie do scrapowania\n",
        "    categories = {\n",
        "        'wszystkie': '/aktualnosci',\n",
        "        'nowości': '/aktualnosci/nowosci',\n",
        "        'zapowiedzi': '/aktualnosci/zapowiedzi',\n",
        "        'wydarzenia': '/aktualnosci/wydarzenia',\n",
        "        'wywiady': '/aktualnosci/wywiady',\n",
        "        'recenzje': '/aktualnosci/recenzje',\n",
        "    }\n",
        "\n",
        "    for cat_name, cat_url in categories.items():\n",
        "        print(f\"Pobieranie linków z kategorii: {cat_name}...\")\n",
        "\n",
        "        # portalkryminalny.pl ma paginację - próbujemy pobrać kilka pierwszych stron\n",
        "        for page in range(1, 6):  # pierwsze 5 stron każdej kategorii\n",
        "            try:\n",
        "                if page == 1:\n",
        "                    url = base_url + cat_url\n",
        "                else:\n",
        "                    # Paginacja to /strona/2, /strona/3 itd.\n",
        "                    url = f\"{base_url}{cat_url}/strona/{page}\"\n",
        "\n",
        "                r = requests.get(url, timeout=10)\n",
        "                r.encoding = 'utf-8'\n",
        "\n",
        "                if r.status_code == 200:\n",
        "                    soup = BeautifulSoup(r.text, 'lxml')\n",
        "\n",
        "                    # Szukamy divów z klasą \"news\" które mają onclick z linkiem\n",
        "                    # <div class=\"news\" onclick=\"location.href='aktualnosci/nowosci/lowca-aurelia-es'\">\n",
        "                    news_divs = soup.find_all('div', class_='news')\n",
        "\n",
        "                    page_links = 0\n",
        "                    for div in news_divs:\n",
        "                        onclick = div.get('onclick', '')\n",
        "                        # Wyciągamy link z onclick=\"location.href='...'\"\n",
        "                        match = re.search(r\"location\\.href='([^']+)'\", onclick)\n",
        "                        if match:\n",
        "                            href = match.group(1)\n",
        "\n",
        "                            # Upewniamy się, że to pełny URL\n",
        "                            if href.startswith('http'):\n",
        "                                full_url = href\n",
        "                            elif href.startswith('/'):\n",
        "                                full_url = base_url + href\n",
        "                            else:\n",
        "                                full_url = base_url + '/' + href\n",
        "\n",
        "                            # Dodajemy tylko linki do artykułów (nie do stron kategorii)\n",
        "                            if '/aktualnosci/' in full_url and '/strona/' not in full_url:\n",
        "                                all_links.add(full_url)\n",
        "                                page_links += 1\n",
        "\n",
        "                    print(f\"  Strona {page}: {page_links} linków\")\n",
        "\n",
        "                    if page_links == 0:\n",
        "                        break  # Jeśli nie ma więcej artykułów, przerywamy paginację\n",
        "\n",
        "                    time.sleep(0.5)  # Aby nie przeciążać serwera\n",
        "\n",
        "                else:\n",
        "                    print(f\"  ✗ Status {r.status_code} dla strony {page}\")\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ✗ Błąd na stronie {page}: {e}\")\n",
        "                break\n",
        "\n",
        "        print(f\"  ✓ Zebrano linki z kategorii {cat_name}\\n\")\n",
        "\n",
        "    # Konwertujemy set na listę\n",
        "    all_links = list(all_links)\n",
        "\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Łącznie znaleziono: {len(all_links)} unikalnych artykułów\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "#%% functions - scraping\n",
        "\n",
        "def dictionary_of_article(article_link):\n",
        "    \"\"\"\n",
        "    Pobiera szczegóły artykułu z portalkryminalny.pl\n",
        "    \"\"\"\n",
        "    try:\n",
        "        r = requests.get(article_link, timeout=15)\n",
        "        r.encoding = 'utf-8'\n",
        "\n",
        "        if r.status_code != 200:\n",
        "            errors.append(article_link)\n",
        "            return\n",
        "\n",
        "        html_text = r.text\n",
        "\n",
        "        if 'error 404' in html_text.lower() or 'page not found' in html_text.lower():\n",
        "            errors.append(article_link)\n",
        "            return\n",
        "\n",
        "        soup = BeautifulSoup(html_text, 'lxml')\n",
        "\n",
        "        # Data publikacji\n",
        "        try:\n",
        "            # W HTML: <p class=\"author\">Autor: <span>materiały wydawcy</span> <br class=\"visible-xs\">Data publikacji: <span>13 stycznia 2026</span></p>\n",
        "            author_p = soup.find('p', class_='author')\n",
        "            if author_p:\n",
        "                date_text = ''\n",
        "                # Szukamy tekstu po \"Data publikacji:\"\n",
        "                text = author_p.get_text()\n",
        "                date_match = re.search(r'Data publikacji:\\s*(.+?)(?:\\n|$)', text)\n",
        "                if date_match:\n",
        "                    date_text = date_match.group(1).strip()\n",
        "            else:\n",
        "                date_text = ''\n",
        "\n",
        "            if date_text:\n",
        "                date_of_publication = date_change_format(date_text)\n",
        "            else:\n",
        "                date_of_publication = \"no date\"\n",
        "        except:\n",
        "            date_of_publication = \"no date\"\n",
        "\n",
        "        # Tytuł\n",
        "        try:\n",
        "            # W HTML: <h1 class=\"text-gold margintop10\">Łowca, Aurelia Es</h1>\n",
        "            title_element = soup.find('h1', class_='text-gold')\n",
        "            if not title_element:\n",
        "                title_element = soup.find('h1')\n",
        "\n",
        "            if title_element:\n",
        "                title = title_element.get_text(strip=True)\n",
        "            else:\n",
        "                title = \"no title\"\n",
        "        except:\n",
        "            title = \"no title\"\n",
        "\n",
        "        # Autor\n",
        "        try:\n",
        "            # W HTML: <p class=\"author\">Autor: <span>materiały wydawcy</span>\n",
        "            author = \"no author\"\n",
        "            author_p = soup.find('p', class_='author')\n",
        "            if author_p:\n",
        "                # Szukamy spana po \"Autor:\"\n",
        "                spans = author_p.find_all('span')\n",
        "                if spans:\n",
        "                    author = spans[0].get_text(strip=True)\n",
        "        except:\n",
        "            author = \"no author\"\n",
        "\n",
        "        # Treść artykułu\n",
        "        try:\n",
        "            # Główna treść jest w <div class=\"col-sm-8\"> wewnątrz <div class=\"col-md-10 col-md-push-1 art\">\n",
        "            article_container = soup.find('div', class_='art')\n",
        "\n",
        "            if article_container:\n",
        "                # Szukamy głównego kontenera z treścią\n",
        "                content_div = article_container.find('div', class_='col-sm-8')\n",
        "\n",
        "                if content_div:\n",
        "                    # Usuń niepotrzebne elementy\n",
        "                    for element in content_div.find_all(['script', 'style', 'iframe', 'ins',\n",
        "                                                         'aside', 'nav', 'header', 'footer',\n",
        "                                                         'div', 'h2', 'h3']):\n",
        "                        # Zostawiamy tylko paragrafy\n",
        "                        if element.name == 'div' and 'buybox' in str(element.get('class', [])):\n",
        "                            element.decompose()\n",
        "                        elif element.name == 'div' and 'bb-widget' in str(element.get('id', '')):\n",
        "                            element.decompose()\n",
        "                        elif element.name in ['h2', 'h3']:\n",
        "                            element.decompose()\n",
        "\n",
        "                    # Wyciągamy tekst tylko z paragrafów\n",
        "                    paragraphs = content_div.find_all('p')\n",
        "                    text_parts = []\n",
        "                    for p in paragraphs:\n",
        "                        p_text = p.get_text(strip=True)\n",
        "                        # Pomijamy paragrafy z cenami i innymi metadanymi\n",
        "                        if p_text and not any(skip in p_text.lower() for skip in\n",
        "                                            ['autor:', 'wydawnictwo:', 'premiera:', 'liczba stron:',\n",
        "                                             'udostępnij', 'sprawdź, gdzie kupić']):\n",
        "                            text_parts.append(p_text)\n",
        "\n",
        "                    text = ' '.join(text_parts)\n",
        "                    text = text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
        "                    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "                else:\n",
        "                    text = \"no text\"\n",
        "            else:\n",
        "                text = \"no text\"\n",
        "        except:\n",
        "            text = \"no text\"\n",
        "\n",
        "        # Kategoria - z URL\n",
        "        try:\n",
        "            # portalkryminalny.pl ma strukturę URL: /aktualnosci/kategoria/tytul\n",
        "            url_parts = article_link.split('/')\n",
        "            if 'aktualnosci' in url_parts:\n",
        "                idx = url_parts.index('aktualnosci')\n",
        "                if len(url_parts) > idx + 1:\n",
        "                    category = url_parts[idx + 1]\n",
        "                    # Kapitalizacja\n",
        "                    category = category.replace('-', ' ').title()\n",
        "                else:\n",
        "                    category = \"no category\"\n",
        "            else:\n",
        "                category = \"no category\"\n",
        "        except:\n",
        "            category = \"no category\"\n",
        "\n",
        "        # Tagi\n",
        "        tags_str = None\n",
        "\n",
        "        # Linki zewnętrzne\n",
        "        try:\n",
        "            article_container = soup.find('div', class_='art')\n",
        "            if article_container:\n",
        "                links = [a['href'] for a in article_container.find_all('a', href=True)]\n",
        "                external_links = [link for link in links\n",
        "                                if not re.search(r'portalkryminalny\\.pl', link)\n",
        "                                and link.startswith('http')\n",
        "                                and 'buybox' not in link\n",
        "                                and 'facebook' not in link\n",
        "                                and 'twitter' not in link]\n",
        "                external_links = ' | '.join(external_links) if external_links else None\n",
        "            else:\n",
        "                external_links = None\n",
        "        except:\n",
        "            external_links = None\n",
        "\n",
        "        # Zdjęcia\n",
        "        try:\n",
        "            images = []\n",
        "\n",
        "            article_container = soup.find('div', class_='art')\n",
        "            if article_container:\n",
        "                # Główne zdjęcie artykułu - w <div class=\"center-block img-responsive\">\n",
        "                main_img = article_container.find('img', class_='shadow-0')\n",
        "                if main_img:\n",
        "                    img_url = main_img.get('src')\n",
        "                    if img_url:\n",
        "                        if not img_url.startswith('http'):\n",
        "                            img_url = 'https://portalkryminalny.pl/' + img_url.lstrip('/')\n",
        "                        if img_url not in images:\n",
        "                            images.append(img_url)\n",
        "\n",
        "                # Wszystkie zdjęcia z treści artykułu\n",
        "                content_div = article_container.find('div', class_='col-sm-8')\n",
        "                if content_div:\n",
        "                    all_imgs = content_div.find_all('img')\n",
        "                    for img in all_imgs:\n",
        "                        img_url = img.get('src')\n",
        "                        if img_url:\n",
        "                            # Pomijamy małe ikony i loga\n",
        "                            if not any(x in img_url.lower() for x in ['icon', 'logo', 'avatar', 'btn', 'social']):\n",
        "                                if img_url not in images:\n",
        "                                    images.append(img_url)\n",
        "\n",
        "            has_images = len(images) > 0\n",
        "            photos_links = ' | '.join(images) if images else None\n",
        "        except:\n",
        "            has_images = False\n",
        "            photos_links = None\n",
        "\n",
        "        # Filmy\n",
        "        try:\n",
        "            has_videos = False\n",
        "            article_container = soup.find('div', class_='art')\n",
        "            if article_container:\n",
        "                iframes = article_container.find_all('iframe', src=True)\n",
        "                has_videos = len(iframes) > 0\n",
        "        except:\n",
        "            has_videos = False\n",
        "\n",
        "        result = {\n",
        "            \"Link\": article_link,\n",
        "            \"Data publikacji\": date_of_publication,\n",
        "            \"Tytuł artykułu\": title.replace('\\xa0', ' '),\n",
        "            \"Tekst artykułu\": text,\n",
        "            \"Autor\": author,\n",
        "            \"Kategoria\": category,\n",
        "            \"Tagi\": tags_str,\n",
        "            \"Linki zewnętrzne\": external_links,\n",
        "            \"Zdjęcia/Grafika\": has_images,\n",
        "            \"Filmy\": has_videos,\n",
        "            \"Linki do zdjęć\": photos_links\n",
        "        }\n",
        "\n",
        "        all_results.append(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        errors.append(article_link)\n",
        "\n",
        "\n",
        "#%% main execution\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SCRAPER PORTALKRYMINALNY.PL\")\n",
        "    print(\"Portal o literaturze kryminalnej\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # KROK 1: Pobierz linki do artykułów\n",
        "    article_links = get_all_article_links()\n",
        "\n",
        "    if not article_links:\n",
        "        print(\"Nie znaleziono artykułów!\")\n",
        "        exit(1)\n",
        "\n",
        "    # KROK 2: Scrapuj artykuły\n",
        "    all_results = []\n",
        "    errors = []\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"KROK 2: Scraping artykułów\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    max_workers = 10\n",
        "    print(f\"Używam {max_workers} równoległych wątków\\n\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        list(tqdm(executor.map(dictionary_of_article, article_links), total=len(article_links)))\n",
        "\n",
        "    # KROK 3: Zapisz wyniki\n",
        "    timestamp = datetime.today().date()\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"KROK 3: Zapisywanie wyników\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # JSON\n",
        "    json_file = f'portalkryminalny_{timestamp}.json'\n",
        "    with open(json_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)\n",
        "    print(f\"  ✓ {json_file}\")\n",
        "\n",
        "    # Excel\n",
        "    excel_file = f\"portalkryminalny_{timestamp}.xlsx\"\n",
        "    df = pd.DataFrame(all_results)\n",
        "    with pd.ExcelWriter(excel_file,\n",
        "                       engine='xlsxwriter',\n",
        "                       engine_kwargs={'options': {'strings_to_urls': False}}) as writer:\n",
        "        df.to_excel(writer, 'Posts', index=False)\n",
        "    print(f\"  ✓ {excel_file}\")\n",
        "\n",
        "    # RAPORT KOŃCOWY\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"RAPORT KOŃCOWY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Pobranych artykułów: {len(all_results)}\")\n",
        "    print(f\"Błędów: {len(errors)}\")\n",
        "\n",
        "    if errors and len(errors) <= 10:\n",
        "        print(f\"\\nLinki z błędami:\")\n",
        "        for error_link in errors:\n",
        "            print(f\"  - {error_link}\")\n",
        "    elif errors:\n",
        "        print(f\"\\nLinki z błędami (pierwsze 10):\")\n",
        "        for error_link in errors[:10]:\n",
        "            print(f\"  - {error_link}\")\n",
        "        print(f\"  ... i {len(errors) - 10} więcej\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"GOTOWE!\")\n",
        "    print(\"=\"*60 + \"\\n\")"
      ]
    }
  ]
}